{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bd090b57-366a-49d1-bf03-1aa6c389d7e8",
   "metadata": {},
   "source": [
    "# PRODUCER"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abb971ff-d791-4817-8609-7b3e07b5abf8",
   "metadata": {},
   "source": [
    "# Análisis de Transacciones en la Plataforma Uber\n",
    "\n",
    "## Universidad Tecnológica de Panamá  \n",
    "### Facultad de Ingeniería de Sistemas Computacionales  \n",
    "### Maestría en Analítica de Datos  \n",
    "\n",
    "**Integrantes:**\n",
    "- Ávila, Isaac  \n",
    "- Sanjur, Andy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f52d301c-69cd-4c76-99fc-0b88aab9d0fd",
   "metadata": {},
   "source": [
    "### 1. Descripción del caso simulado\n",
    "\n",
    "En la última década, las plataformas de solicitud de viajes han adquirido una relevancia significativa en el ámbito de la movilidad urbana. Gracias a su practicidad, cada vez más usuarios y conductores optan por este tipo de soluciones de transporte privado. En una ciudad tan grande como Nueva York (NYC), pueden registrarse cientos de viajes por minuto, lo cual representa un reto importante para el análisis de datos en tiempo real mediante métodos convencionales, o bien vuelve ineficiente el análisis de datos históricos para el ajuste dinámico de tarifas según la demanda del momento.\n",
    "\n",
    "El siguiente caso simulado presenta una serie de transacciones realizadas por la plataforma Uber. Cada registro corresponde a un viaje efectuado desde un punto A hasta un punto B, incluyendo su respectivo detalle. A partir de estos datos, se busca analizar los siguientes aspectos:\n",
    "\n",
    "- ¿Cuáles son los puntos de partida más comunes?\n",
    "- ¿Cuáles son los destinos más concurridos?\n",
    "- ¿En qué días y a qué horas existe mayor frecuencia de solicitudes de viaje?\n",
    "- Determinar cuáles son las matrículas con mayor cantidad de demoras.\n",
    "- Determinar los minutos promedio de demora.\n",
    "\n",
    "Para el análisis se utiliza un conjunto de datos en formato `.csv`, recopilado de la plataforma Kaggle:  \n",
    "https://www.kaggle.com/datasets/ahmedramadan74/uber-nyc  \n",
    "\n",
    "Este conjunto de datos incluye los siguientes campos relevantes para el caso de uso:\n",
    "\n",
    "- **index_trip (int):** Número único del evento.\n",
    "- **hvfhs_license_num (str):** Tipo de servicio contratado.\n",
    "- **request_datetime (timestamp):** Fecha y hora en que se solicita el servicio.\n",
    "- **pickup_datetime (timestamp):** Fecha y hora de inicio del viaje.\n",
    "- **dropoff_datetime (timestamp):** Fecha y hora de finalización del viaje.\n",
    "- **PULocationID (int):** Zona donde inicia el viaje.\n",
    "- **DOLocationID (int):** Zona donde finaliza el viaje.\n",
    "- **trip_miles (double):** Distancia total del viaje.\n",
    "- **trip_time (int):** Duración del viaje en segundos.\n",
    "- **base_passenger_fare (double):** Tarifa base del viaje.\n",
    "- **tolls (double):** Total de peajes.\n",
    "- **sales_tax (double):** Total de impuestos.\n",
    "- **congestion_surcharge (double):** Costos adicionales por congestión vehicular.\n",
    "- **airport_fee (double):** Tarifa adicional por aeropuerto (USD 2.50).\n",
    "- **tips (double):** Propinas.\n",
    "- **driver_pay (double):** Pago al conductor.\n",
    "- **hour (double):** Hora de la solicitud.\n",
    "- **year (double):** Año de la solicitud.\n",
    "- **month (double):** Mes de la solicitud.\n",
    "- **day (double):** Día de la solicitud.\n",
    "- **on_time_pickup (int):** Indicador de llegada a tiempo.\n",
    "\n",
    "---\n",
    "\n",
    "### 2. Justificación del uso de Big Data en el problema\n",
    "\n",
    "La alta demanda y el gran volumen de información que puede generar la aplicación durante las horas pico dificultan el análisis en tiempo real mediante herramientas tradicionales de analítica de datos. En este contexto, las oportunidades de negocio pueden perder relevancia si la información no es procesada y utilizada de manera inmediata.\n",
    "\n",
    "Para este caso, con el objetivo de mantener la rapidez en el análisis y manejar eficientemente el volumen de datos que se generan cada minuto, se emplean tecnologías orientadas a Big Data, tales como **Apache Cassandra** para el almacenamiento distribuido de datos, **Apache Kafka** para la transmisión de datos en tiempo real (streaming) y **PySpark** para la transformación, consulta y exportación de la información analizada."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85e3d60e-6044-4752-a283-454612c39c4c",
   "metadata": {},
   "source": [
    "### 3. Proceso utilizado para el análisis\n",
    "\n",
    "#### 3.1 Creación de la sesión de Spark\n",
    "\n",
    "En esta etapa se importa la librería **SparkSession** y se crea la sesión de Apache Spark.  \n",
    "Para ello, se configura el **host** y el **puerto** de conexión que permitirán establecer la comunicación con el entorno de procesamiento distribuido.\n",
    "\n",
    "##### 3.1.1 Definición de constantes de configuración\n",
    "\n",
    "Con el fin de mejorar la mantenibilidad y portabilidad del código, se definen constantes que agrupan los parámetros de configuración de la sesión de Spark y la conexión con la base de datos Apache Cassandra. Este enfoque permite modificar fácilmente los valores de conexión sin afectar la lógica del procesamiento del análisis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2dded08-5156-420d-9538-8973cee6db7d",
   "metadata": {},
   "source": [
    "\n",
    "### 1. Definición de constantes de configuración\n",
    "\n",
    "Con el fin de mejorar la mantenibilidad y portabilidad del código, se definen constantes que agrupan los parámetros de configuración de la sesión de Spark y la conexión con la base de datos Apache Cassandra. Este enfoque permite modificar fácilmente los valores de conexión sin afectar la lógica del procesamiento del análisis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1fcbbc23-6c8a-4bb9-8303-bb673be1d31d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from cassandra.cluster import Cluster\n",
    "from cassandra.policies import RoundRobinPolicy\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "# =======================================================\n",
    "# 1. CONSTANTES DE CONEXIÓN Y CONFIGURACIÓN\n",
    "# =======================================================\n",
    "\n",
    "# Hosts de los nodos de Cassandra\n",
    "CASSANDRA_HOST_ANDY = \"localhost\"\n",
    "CASSANDRA_PORT = \"9042\"\n",
    "\n",
    "# Nombre de la aplicación Spark\n",
    "APP_NAME = \"Proyecto_Final_Uber_Trips\"\n",
    "\n",
    "# Conector de Spark-Cassandra\n",
    "CASSANDRA_CONNECTOR = \"com.datastax.spark:spark-cassandra-connector_2.13:3.5.1\"\n",
    "\n",
    "# Configuración del Keyspace y la Tabla en Cassandra\n",
    "KEYSPACE = \"trips\"\n",
    "TABLE_RAW = \"uber_trips\"\n",
    "TABLE_CLEAN = \"uber_trips_clean\"\n",
    "TABLE_PROD = \"uber_trips_prod\"\n",
    "REPLICATION_FACTOR = 1\n",
    "BORRAR_DATOS = False\n",
    "# Si el error persiste, dejar en True para borrar y recrear la tabla con el esquema correcto (minúsculas).\n",
    "FORZAR_RECREACION_ESQUEMA = True\n",
    "\n",
    "# Definición de las 33 columnas que deben existir en la tabla final de Cassandra, en minúsculas.\n",
    "COLUMNAS_FINALES_CASSANDRA = [\n",
    "    \"index_trip\", \"hvfhs_license_num\", \"dispatching_base_num\", \"originating_base_num\",\n",
    "    \"request_datetime\", \"on_scene_datetime\", \"pickup_datetime\", \"dropoff_datetime\",\n",
    "    \"pulocationid\", \"dolocationid\", \"trip_miles\", \"trip_time\", \"base_passenger_fare\",\n",
    "    \"tolls\", \"bcf\", \"sales_tax\", \"congestion_surcharge\", \"airport_fee\", \"tips\",\n",
    "    \"driver_pay\", \"shared_request_flag\", \"shared_match_flag\", \"access_a_ride_flag\",\n",
    "    \"wav_request_flag\", \"wav_match_flag\", \"hour\", \"year\", \"month\", \"day\",\n",
    "    \"on_time_pickup\",\n",
    "    \"uber_sales\",\n",
    "    \"pickup_zone\",\n",
    "    \"delivery_zone\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebada54c-0b50-4e54-bcf3-7ec85e44a4ff",
   "metadata": {},
   "source": [
    "#### 2 Creación de la sesión de Spark\n",
    "\n",
    "En esta etapa se importa la librería **SparkSession** y se crea la sesión de Apache Spark.  \n",
    "Para ello, se configura el **host** y el **puerto** de conexión que permitirán establecer la comunicación con el entorno de procesamiento distribuido."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "af93e901-8692-4514-8512-43954676dd00",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Using incubator modules: jdk.incubator.vector\n",
      ":: loading settings :: url = jar:file:/home/admin/jupyter_venv/lib/python3.12/site-packages/pyspark/jars/ivy-2.5.3.jar!/org/apache/ivy/core/settings/ivysettings.xml\n",
      "Ivy Default Cache set to: /home/admin/.ivy2.5.2/cache\n",
      "The jars for the packages stored in: /home/admin/.ivy2.5.2/jars\n",
      "com.datastax.spark#spark-cassandra-connector_2.13 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-4babbce5-578f-48ba-b4ba-55fcc627fde6;1.0\n",
      "\tconfs: [default]\n",
      "\tfound com.datastax.spark#spark-cassandra-connector_2.13;3.5.1 in central\n",
      "\tfound com.datastax.spark#spark-cassandra-connector-driver_2.13;3.5.1 in central\n",
      "\tfound org.scala-lang.modules#scala-collection-compat_2.13;2.11.0 in central\n",
      "\tfound org.scala-lang.modules#scala-parallel-collections_2.13;1.0.4 in central\n",
      "\tfound org.apache.cassandra#java-driver-core-shaded;4.18.1 in central\n",
      "\tfound com.datastax.oss#native-protocol;1.5.1 in central\n",
      "\tfound com.datastax.oss#java-driver-shaded-guava;25.1-jre-graal-sub-1 in central\n",
      "\tfound com.typesafe#config;1.4.1 in central\n",
      "\tfound org.slf4j#slf4j-api;1.7.26 in central\n",
      "\tfound io.dropwizard.metrics#metrics-core;4.1.18 in central\n",
      "\tfound org.hdrhistogram#HdrHistogram;2.1.12 in central\n",
      "\tfound org.reactivestreams#reactive-streams;1.0.3 in central\n",
      "\tfound org.apache.cassandra#java-driver-mapper-runtime;4.18.1 in central\n",
      "\tfound org.apache.cassandra#java-driver-query-builder;4.18.1 in central\n",
      "\tfound org.apache.commons#commons-lang3;3.10 in central\n",
      "\tfound com.thoughtworks.paranamer#paranamer;2.8 in central\n",
      "\tfound org.scala-lang#scala-reflect;2.13.13 in central\n",
      ":: resolution report :: resolve 469ms :: artifacts dl 10ms\n",
      "\t:: modules in use:\n",
      "\tcom.datastax.oss#java-driver-shaded-guava;25.1-jre-graal-sub-1 from central in [default]\n",
      "\tcom.datastax.oss#native-protocol;1.5.1 from central in [default]\n",
      "\tcom.datastax.spark#spark-cassandra-connector-driver_2.13;3.5.1 from central in [default]\n",
      "\tcom.datastax.spark#spark-cassandra-connector_2.13;3.5.1 from central in [default]\n",
      "\tcom.thoughtworks.paranamer#paranamer;2.8 from central in [default]\n",
      "\tcom.typesafe#config;1.4.1 from central in [default]\n",
      "\tio.dropwizard.metrics#metrics-core;4.1.18 from central in [default]\n",
      "\torg.apache.cassandra#java-driver-core-shaded;4.18.1 from central in [default]\n",
      "\torg.apache.cassandra#java-driver-mapper-runtime;4.18.1 from central in [default]\n",
      "\torg.apache.cassandra#java-driver-query-builder;4.18.1 from central in [default]\n",
      "\torg.apache.commons#commons-lang3;3.10 from central in [default]\n",
      "\torg.hdrhistogram#HdrHistogram;2.1.12 from central in [default]\n",
      "\torg.reactivestreams#reactive-streams;1.0.3 from central in [default]\n",
      "\torg.scala-lang#scala-reflect;2.13.13 from central in [default]\n",
      "\torg.scala-lang.modules#scala-collection-compat_2.13;2.11.0 from central in [default]\n",
      "\torg.scala-lang.modules#scala-parallel-collections_2.13;1.0.4 from central in [default]\n",
      "\torg.slf4j#slf4j-api;1.7.26 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   17  |   0   |   0   |   0   ||   17  |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-4babbce5-578f-48ba-b4ba-55fcc627fde6\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 17 already retrieved (0kB/7ms)\n",
      "25/12/15 23:56:09 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession iniciada y conectada a Cassandra.\n"
     ]
    }
   ],
   "source": [
    "# =======================================================\n",
    "# 2. INICIALIZACIÓN DE SPARK\n",
    "# =======================================================\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(APP_NAME) \\\n",
    "    .config(\"spark.jars.packages\", CASSANDRA_CONNECTOR) \\\n",
    "    .config(\"spark.cassandra.connection.host\", CASSANDRA_HOST_ANDY) \\\n",
    "    .config(\"spark.cassandra.connection.port\", CASSANDRA_PORT) \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(\"SparkSession iniciada y conectada a Cassandra.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ba48944-2a2e-4f8d-a7ba-ef89cc57d2b0",
   "metadata": {},
   "source": [
    "### 3 Integración con Cassandra\n",
    "\n",
    "En esta etapa se realiza la creación y conexión al clúster de **Apache Cassandra**, el cual es utilizado como sistema de almacenamiento distribuido. Se define el **keyspace** denominado `trips` y se crea la tabla `uber_trips`, estructurada de acuerdo con las columnas del conjunto de datos analizado. Finalmente, se efectúa la escritura de la información en Cassandra, persistiendo los datos previamente cargados y procesados mediante Apache Spark.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0675969b-a853-44d2-8af1-30a090fe5585",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conexión con Cassandra establecida.\n",
      "Keyspace 'trips' verificado/creado.\n",
      "\n",
      "ADVERTENCIA: FORZANDO RECREACIÓN DE ESQUEMAS PARA CORREGIR PROBLEMAS DE CASE SENSITIVITY.\n",
      "Tablas eliminadas. Procediendo a recrear.\n",
      "Tabla de datos crudos 'uber_trips' verificado/creada.\n",
      "Tabla de datos limpios 'uber_trips_clean' verificado/creada.\n",
      "Tabla de producción 'uber_trips_prod' verificado/creada.\n",
      "\n",
      "IMPORTANTE: Una vez que el proceso sea exitoso, cambia 'FORZAR_RECREACION_ESQUEMA = False'.\n",
      "Borrado de datos (TRUNCATE) deshabilitado. Se procederá a DEPOSITAR los datos (APPEND).\n"
     ]
    }
   ],
   "source": [
    "# =======================================================\n",
    "# 3. CONEXIÓN Y DEFINICIÓN DE ESQUEMAS DE CASSANDRA\n",
    "# =======================================================\n",
    "\n",
    "\n",
    "cluster = Cluster(\n",
    "    contact_points=[CASSANDRA_HOST_ANDY],\n",
    "    port=int(CASSANDRA_PORT),\n",
    "    load_balancing_policy=RoundRobinPolicy()\n",
    ")\n",
    "\n",
    "session = cluster.connect()\n",
    "print(\"Conexión con Cassandra establecida.\")\n",
    "\n",
    "# Crear Keyspace (si no existe)\n",
    "session.execute(f\"\"\"\n",
    "CREATE KEYSPACE IF NOT EXISTS {KEYSPACE}\n",
    "WITH replication = {{\n",
    "    'class': 'SimpleStrategy',\n",
    "    'replication_factor': {REPLICATION_FACTOR}\n",
    "}};\n",
    "\"\"\")\n",
    "session.set_keyspace(KEYSPACE)\n",
    "print(f\"Keyspace '{KEYSPACE}' verificado/creado.\")\n",
    "\n",
    "\n",
    "# **PASO CRÍTICO DE LIMPIEZA DE ESQUEMA**\n",
    "if FORZAR_RECREACION_ESQUEMA:\n",
    "    print(\"\\nADVERTENCIA: FORZANDO RECREACIÓN DE ESQUEMAS PARA CORREGIR PROBLEMAS DE CASE SENSITIVITY.\")\n",
    "    session.execute(f\"DROP TABLE IF EXISTS {TABLE_RAW};\")\n",
    "    session.execute(f\"DROP TABLE IF EXISTS {TABLE_CLEAN};\")\n",
    "    session.execute(f\"DROP TABLE IF EXISTS {TABLE_PROD};\")\n",
    "    print(\"Tablas eliminadas. Procediendo a recrear.\")\n",
    "\n",
    "\n",
    "# 3.1. Creación de la tabla de datos crudos (Raw Table: uber_trips)\n",
    "RAW_TABLE_SCHEMA = \"\"\"\n",
    "    index_trip int PRIMARY KEY, hvfhs_license_num text, dispatching_base_num text,\n",
    "    originating_base_num text, request_datetime timestamp, on_scene_datetime timestamp,\n",
    "    pickup_datetime timestamp, dropoff_datetime timestamp, pulocationid int, dolocationid int,\n",
    "    trip_miles float, trip_time int, base_passenger_fare float, tolls float, bcf float,\n",
    "    sales_tax float, congestion_surcharge float, airport_fee float, tips float, driver_pay float,\n",
    "    shared_request_flag text, shared_match_flag text, access_a_ride_flag text,\n",
    "    wav_request_flag text, wav_match_flag text, hour float, year float, month float, day float,\n",
    "    on_time_pickup int\n",
    "\"\"\"\n",
    "session.execute(f\"CREATE TABLE IF NOT EXISTS {TABLE_RAW} ({RAW_TABLE_SCHEMA});\")\n",
    "print(f\"Tabla de datos crudos '{TABLE_RAW}' verificado/creada.\")\n",
    "\n",
    "# 3.2. Creación de la tabla de datos limpios (Clean Table: uber_trips_clean)\n",
    "CLEAN_TABLE_SCHEMA = RAW_TABLE_SCHEMA + \", uber_sales float, pickup_zone text, delivery_zone text\"\n",
    "session.execute(f\"CREATE TABLE IF NOT EXISTS {TABLE_CLEAN} ({CLEAN_TABLE_SCHEMA});\")\n",
    "print(f\"Tabla de datos limpios '{TABLE_CLEAN}' verificado/creada.\")\n",
    "\n",
    "# 3.3. Creación de la tabla de datos de producción (Prod Table: uber_trips_prod)\n",
    "session.execute(f\"CREATE TABLE IF NOT EXISTS {TABLE_PROD} ({CLEAN_TABLE_SCHEMA});\")\n",
    "print(f\"Tabla de producción '{TABLE_PROD}' verificado/creada.\")\n",
    "\n",
    "if FORZAR_RECREACION_ESQUEMA:\n",
    "    print(\"\\nIMPORTANTE: Una vez que el proceso sea exitoso, cambia 'FORZAR_RECREACION_ESQUEMA = False'.\")\n",
    "\n",
    "# 3.4. TRUNCATE de Tablas (Operación Desactivada)\n",
    "if BORRAR_DATOS:\n",
    "    pass\n",
    "else:\n",
    "    print(\"Borrado de datos (TRUNCATE) deshabilitado. Se procederá a DEPOSITAR los datos (APPEND).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "456d40a1-84ee-4766-a196-66e90ee71b51",
   "metadata": {},
   "source": [
    "#### 4. Lectura e importación del conjunto de datos\n",
    "\n",
    "En esta etapa se realiza la lectura del conjunto de datos en formato `.csv` utilizando PySpark. Posteriormente, se visualizan las primeras cinco filas del conjunto de registros con el fin de validar la correcta carga de la información. Adicionalmente, se emplea la función `printSchema()` para inspeccionar la estructura del esquema y los tipos de datos asociados a cada columna."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a6aedf62-c83c-46d2-bebd-992744396889",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- ESQUEMA LEÍDO DEL CSV (raw_trips) ---\n",
      "root\n",
      " |-- index_trip: integer (nullable = true)\n",
      " |-- hvfhs_license_num: string (nullable = true)\n",
      " |-- dispatching_base_num: string (nullable = true)\n",
      " |-- originating_base_num: string (nullable = true)\n",
      " |-- request_datetime: timestamp (nullable = true)\n",
      " |-- on_scene_datetime: timestamp (nullable = true)\n",
      " |-- pickup_datetime: timestamp (nullable = true)\n",
      " |-- dropoff_datetime: timestamp (nullable = true)\n",
      " |-- PULocationID: integer (nullable = true)\n",
      " |-- DOLocationID: integer (nullable = true)\n",
      " |-- trip_miles: double (nullable = true)\n",
      " |-- trip_time: integer (nullable = true)\n",
      " |-- base_passenger_fare: double (nullable = true)\n",
      " |-- tolls: double (nullable = true)\n",
      " |-- bcf: double (nullable = true)\n",
      " |-- sales_tax: double (nullable = true)\n",
      " |-- congestion_surcharge: double (nullable = true)\n",
      " |-- airport_fee: double (nullable = true)\n",
      " |-- tips: double (nullable = true)\n",
      " |-- driver_pay: double (nullable = true)\n",
      " |-- shared_request_flag: string (nullable = true)\n",
      " |-- shared_match_flag: string (nullable = true)\n",
      " |-- access_a_ride_flag: string (nullable = true)\n",
      " |-- wav_request_flag: string (nullable = true)\n",
      " |-- wav_match_flag: string (nullable = true)\n",
      " |-- hour: double (nullable = true)\n",
      " |-- year: double (nullable = true)\n",
      " |-- month: double (nullable = true)\n",
      " |-- day: double (nullable = true)\n",
      " |-- on_time_pickup: integer (nullable = true)\n",
      "\n",
      "------------------------------------------\n",
      "Columnas PULocationID/DOLocationID renombradas a pulocationid/dolocationid.\n",
      "\n",
      "--- Iniciando DEPOSITO de datos iniciales a uber_trips (Modo APPEND) ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/12/15 23:56:29 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datos iniciales (1000 filas) DEPOSITADOS en la tabla 'uber_trips'.\n"
     ]
    }
   ],
   "source": [
    "# =======================================================\n",
    "# 4. LECTURA Y CARGA INICIAL (CSV -> raw_trips)\n",
    "# =======================================================\n",
    "\n",
    "raw_trips = spark.read.csv(\n",
    "    \"data/sampling_trips.csv\",\n",
    "    header=True,\n",
    "    inferSchema=True\n",
    ").limit(1000)\n",
    "\n",
    "print(\"\\n--- ESQUEMA LEÍDO DEL CSV (raw_trips) ---\")\n",
    "raw_trips.printSchema() # Muestra las columnas con mayúsculas/camelCase\n",
    "print(\"------------------------------------------\")\n",
    "\n",
    "\n",
    "# **CORRECCIÓN 1: Renombrar a minúsculas ANTES de la primera carga**\n",
    "raw_trips = raw_trips.withColumnRenamed(\"PULocationID\", \"pulocationid\") \\\n",
    "                     .withColumnRenamed(\"DOLocationID\", \"dolocationid\")\n",
    "\n",
    "print(\"Columnas PULocationID/DOLocationID renombradas a pulocationid/dolocationid.\")\n",
    "\n",
    "print(\"\\n--- Iniciando DEPOSITO de datos iniciales a uber_trips (Modo APPEND) ---\")\n",
    "raw_trips.repartition(10) \\\n",
    "    .write \\\n",
    "    .format(\"org.apache.spark.sql.cassandra\") \\\n",
    "    .mode(\"append\") \\\n",
    "    .options(table=TABLE_RAW, keyspace=KEYSPACE) \\\n",
    "    .save()\n",
    "print(f\"Datos iniciales ({raw_trips.count()} filas) DEPOSITADOS en la tabla '{TABLE_RAW}'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "080b45f0-a91e-4022-8654-306aa0915719",
   "metadata": {},
   "source": [
    "#### 5. Consulta de la tabla en Cassandra\n",
    "\n",
    "En esta etapa se valida la conexión con **Apache Cassandra** mediante la lectura de los datos almacenados desde Apache Spark. Se realiza la consulta de la tabla `uber_trips`, perteneciente al keyspace `trips`, y los resultados se cargan en un DataFrame denominado `df_raw`. Finalmente, se visualizan las primeras diez filas con el objetivo de verificar la correcta persistencia y recuperación de la información."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "222fc250-830d-457d-902b-0c946222473a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lectura de df_raw desde Cassandra exitosa. Filas leídas: 1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "# =======================================================\n",
    "# 5. LECTURA DE DATOS DESDE CASSANDRA (READ)\n",
    "# =======================================================\n",
    "\n",
    "df_raw = spark.read \\\n",
    "    .format(\"org.apache.spark.sql.cassandra\") \\\n",
    "    .options(table=TABLE_RAW, keyspace=KEYSPACE) \\\n",
    "    .load()\n",
    "    \n",
    "filas_iniciales = df_raw.count()\n",
    "print(f\"Lectura de df_raw desde Cassandra exitosa. Filas leídas: {filas_iniciales}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd278ac0-d1e0-4b25-95cd-9327f863a788",
   "metadata": {},
   "source": [
    "#### 6. Transformación del conjunto de datos\n",
    "\n",
    "Se realiza la lectura del archivo `.csv` con la información de las ubicaciones y se aplican transformaciones de tipo `JOIN` entre tablas. Posteriormente, se ejecutan cálculos entre columnas para determinar la comisión de la plataforma y se generan agregaciones temporales. Finalmente, los datos procesados y agregados son exportados en formato `.csv`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9acbfe62-248d-4a41-9d48-cc5364396b60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Iniciando Proceso de Transformación y Limpieza ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG: Filas después de limpieza específica (base_fare, pu_id): 1000\n",
      "DEBUG: Filas después de Joins (debería ser el mismo conteo): 1000\n",
      "\n",
      "--- 6.1: Validación Final de Esquema para Cassandra ---\n",
      "Validación de columnas para Cassandra OK. Listo para depositar.\n"
     ]
    }
   ],
   "source": [
    "# =======================================================\n",
    "# 6. TRANSFORMACIÓN, LIMPIEZA Y ENRIQUECIMIENTO\n",
    "# =======================================================\n",
    "\n",
    "print(\"\\n--- Iniciando Proceso de Transformación y Limpieza ---\")\n",
    "\n",
    "# --- PASO CRÍTICO DE LIMPIEZA ---\n",
    "# ⚠️ CORRECCIÓN: Evitamos .dropna() global. Filtramos solo por columnas clave.\n",
    "# Aseguramos que la columna de tarifa base exista, ya que es la clave del cálculo de ventas.\n",
    "df_clean = df_raw.filter(col(\"base_passenger_fare\").isNotNull()) \n",
    "# También aseguramos que las IDs de ubicación existan\n",
    "df_clean = df_clean.filter(col(\"pulocationid\").isNotNull()) \n",
    "\n",
    "filas_despues_limpieza = df_clean.count()\n",
    "print(f\"DEBUG: Filas después de limpieza específica (base_fare, pu_id): {filas_despues_limpieza}\")\n",
    "\n",
    "\n",
    "# CÁLCULO uber_sales\n",
    "df_clean = df_clean.withColumn(\n",
    "    \"uber_sales\",\n",
    "    col(\"base_passenger_fare\") + col(\"tips\") - col(\"driver_pay\")\n",
    ")\n",
    "\n",
    "# LECTURA DE ZONAS (Lookup Table)\n",
    "loc = spark.read \\\n",
    "    .csv(\"data/taxi_zone_lookup.csv\", header=True, inferSchema=True) \\\n",
    "    .drop(\"Borough\", \"service_zone\")\n",
    "\n",
    "# Preparamos las tablas para el join \n",
    "loc_pickup = loc.withColumnRenamed(\"Zone\", \"pickup_zone\").withColumnRenamed(\"LocationID\", \"PULocationID_lookup\")\n",
    "loc_dropoff = loc.withColumnRenamed(\"Zone\", \"delivery_zone\").withColumnRenamed(\"LocationID\", \"DOLocationID_lookup\")\n",
    "\n",
    "# JOINS (Enriquecimiento de datos)\n",
    "# 1. Join para obtener el nombre de la zona de recogida\n",
    "df_clean = df_clean.join(\n",
    "    loc_pickup,\n",
    "    col(\"pulocationid\") == col(\"PULocationID_lookup\"),\n",
    "    \"left\"\n",
    ").drop(col(\"PULocationID_lookup\")) \n",
    "\n",
    "# 2. Join para obtener el nombre de la zona de destino\n",
    "df_clean = df_clean.join(\n",
    "    loc_dropoff,\n",
    "    col(\"dolocationid\") == col(\"DOLocationID_lookup\"),\n",
    "    \"left\"\n",
    ").drop(col(\"DOLocationID_lookup\")) \n",
    "\n",
    "filas_despues_joins = df_clean.count()\n",
    "print(f\"DEBUG: Filas después de Joins (debería ser el mismo conteo): {filas_despues_joins}\")\n",
    "\n",
    "\n",
    "# AGREGACIÓN\n",
    "df_agg_uber_sales = df_clean.groupBy(\n",
    "    \"year\", \"month\", \"day\", \"hour\"\n",
    ").sum(\"uber_sales\").withColumnRenamed(\"sum(uber_sales)\", \"total_uber_sales\")\n",
    "\n",
    "\n",
    "# **PASO CRÍTICO 6.1: VALIDACIÓN Y SELECCIÓN FINAL DE COLUMNAS**\n",
    "print(\"\\n--- 6.1: Validación Final de Esquema para Cassandra ---\")\n",
    "\n",
    "# Filtrar el DataFrame para incluir SOLO las columnas de Cassandra\n",
    "df_clean_final = df_clean.select(*COLUMNAS_FINALES_CASSANDRA)\n",
    "df_clean = df_clean_final\n",
    "print(\"Validación de columnas para Cassandra OK. Listo para depositar.\")\n",
    "\n",
    "# **CONTEO DE FILAS FINALES**\n",
    "filas_depositadas = df_clean.count()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc373ecd-96af-48a8-90a5-d5c9368f48a7",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a1b29c72-3432-46dc-b918-20b2f102e516",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- DEPOSITANDO datos limpios a uber_trips_clean ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RESULTADO: 1000 filas DEPOSITADAS en la tabla 'uber_trips_clean'.\n",
      "--- DEPOSITANDO datos limpios a uber_trips_prod ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 30:=============================================>          (25 + 6) / 31]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RESULTADO: 1000 filas DEPOSITADAS en la tabla 'uber_trips_prod'.\n",
      "\n",
      "Datos DEPOSITADOS correctamente en uber_trips, uber_trips_clean y uber_trips_prod.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "# =======================================================\n",
    "# 7. EXPORTACIÓN DE DATOS PROCESADOS A CASSANDRA\n",
    "# =======================================================\n",
    "\n",
    "\n",
    "# Carga a uber_trips_clean (DEPOSITO en modo APPEND)\n",
    "print(\"\\n--- DEPOSITANDO datos limpios a uber_trips_clean ---\")\n",
    "df_clean.write \\\n",
    "    .format(\"org.apache.spark.sql.cassandra\") \\\n",
    "    .mode(\"append\") \\\n",
    "    .options(table=TABLE_CLEAN, keyspace=KEYSPACE) \\\n",
    "    .save()\n",
    "\n",
    "print(f\"RESULTADO: {filas_depositadas} filas DEPOSITADAS en la tabla '{TABLE_CLEAN}'.\")\n",
    "\n",
    "# Carga a uber_trips_prod (DEPOSITO en modo APPEND)\n",
    "print(\"--- DEPOSITANDO datos limpios a uber_trips_prod ---\")\n",
    "df_clean.write \\\n",
    "    .format(\"org.apache.spark.sql.cassandra\") \\\n",
    "    .mode(\"append\") \\\n",
    "    .options(table=TABLE_PROD, keyspace=KEYSPACE) \\\n",
    "    .save()\n",
    "\n",
    "print(f\"RESULTADO: {filas_depositadas} filas DEPOSITADAS en la tabla '{TABLE_PROD}'.\")\n",
    "\n",
    "print(\"\\nDatos DEPOSITADOS correctamente en uber_trips, uber_trips_clean y uber_trips_prod.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a63c963e-842f-49c5-aa2a-6242b1bad4cc",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "694a5e3c-0673-415a-8e1e-df448d81722b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 34:=====================================>                 (21 + 10) / 31]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datos limpios y agregados exportados a CSV.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "# =======================================================\n",
    "# 8. EXPORTACIÓN A CSV (Resultados Agregados)\n",
    "# =======================================================\n",
    "\n",
    "os.makedirs(\"output/clean_data\", exist_ok=True)\n",
    "os.makedirs(\"output/aggregated_data\", exist_ok=True)\n",
    "\n",
    "df_clean.write.mode(\"overwrite\").csv(\"output/clean_data\", header=True)\n",
    "df_agg_uber_sales.write.mode(\"overwrite\").csv(\"output/aggregated_data\", header=True)\n",
    "print(\"Datos limpios y agregados exportados a CSV.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd60891c-bf96-45d8-8e32-bc15eba5d7a3",
   "metadata": {},
   "source": [
    "### cierre de conexiòn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e57b592c-a2dd-40df-898a-5aa7ebf870b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Proceso ETL Completado. Conexiones a Spark y Cassandra cerradas. ---\n"
     ]
    }
   ],
   "source": [
    "# =======================================================\n",
    "# 9. CIERRE DE CONEXIONES\n",
    "# =======================================================\n",
    "\n",
    "session.shutdown()\n",
    "cluster.shutdown()\n",
    "spark.stop()\n",
    "print(\"\\n--- Proceso ETL Completado. Conexiones a Spark y Cassandra cerradas. ---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e16ce3d-47b0-42a6-a827-431c16cf0544",
   "metadata": {},
   "source": [
    "<b>Configuración del productor en Kafka (Tabla cruda)</b>\n",
    "<br>\n",
    "Se establece la conexión con Cassandra para consultar la tabla `uber_trips`, almacenando los registros en una variable. Posteriormente, se importan las librerías necesarias para Kafka y JSON con el fin de serializar los datos y establecer la conexión con el servicio. Finalmente, se empaquetan las columnas requeridas en un diccionario y se envían al tópico `uber_trips` con un intervalo de un segundo entre cada transacción."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7b5fad03-2336-4e90-9e07-1d2bb5e4616d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conexión con Cassandra establecida en el host: 100.68.89.127\n",
      "Conexión a Kafka Broker establecida: 100.68.89.127:9092\n",
      "\n",
      "-> Producer uber_trips_clean iniciado\n",
      "\n",
      "-> Producer uber_trips_prod iniciado\n",
      "-> uber_trips_clean: index_trip 769 enviado. Total: 1\n",
      "-> uber_trips_prod: index_trip 769 enviado. Total: 1\n",
      "-> uber_trips_clean: index_trip 23 enviado. Total: 2\n",
      "-> uber_trips_prod: index_trip 23 enviado. Total: 2\n",
      "-> uber_trips_prod: index_trip 114 enviado. Total: 3\n",
      "-> uber_trips_clean: index_trip 114 enviado. Total: 3\n",
      "-> uber_trips_prod: index_trip 660 enviado. Total: 4\n",
      "-> uber_trips_clean: index_trip 660 enviado. Total: 4\n",
      "-> uber_trips_prod: index_trip 893 enviado. Total: 5-> uber_trips_clean: index_trip 893 enviado. Total: 5\n",
      "\n",
      "-> uber_trips_prod: index_trip 53 enviado. Total: 6\n",
      "-> uber_trips_clean: index_trip 53 enviado. Total: 6\n",
      "-> uber_trips_prod: index_trip 987 enviado. Total: 7\n",
      "-> uber_trips_clean: index_trip 987 enviado. Total: 7\n",
      "-> uber_trips_prod: index_trip 878 enviado. Total: 8\n",
      "-> uber_trips_clean: index_trip 878 enviado. Total: 8\n",
      "-> uber_trips_prod: index_trip 110 enviado. Total: 9\n",
      "-> uber_trips_clean: index_trip 110 enviado. Total: 9\n",
      "-> uber_trips_prod: index_trip 91 enviado. Total: 10\n",
      "-> uber_trips_clean: index_trip 91 enviado. Total: 10\n",
      "-> uber_trips_prod: index_trip 128 enviado. Total: 11\n",
      "-> uber_trips_clean: index_trip 128 enviado. Total: 11\n",
      "-> uber_trips_prod: index_trip 363 enviado. Total: 12-> uber_trips_clean: index_trip 363 enviado. Total: 12\n",
      "\n",
      "-> uber_trips_clean: index_trip 251 enviado. Total: 13\n",
      "-> uber_trips_prod: index_trip 251 enviado. Total: 13\n",
      "-> uber_trips_prod: index_trip 744 enviado. Total: 14\n",
      "-> uber_trips_clean: index_trip 744 enviado. Total: 14\n",
      "-> uber_trips_prod: index_trip 778 enviado. Total: 15\n",
      "-> uber_trips_clean: index_trip 778 enviado. Total: 15\n",
      "-> uber_trips_prod: index_trip 819 enviado. Total: 16\n",
      "-> uber_trips_clean: index_trip 819 enviado. Total: 16\n",
      "-> uber_trips_clean: index_trip 310 enviado. Total: 17-> uber_trips_prod: index_trip 310 enviado. Total: 17\n",
      "\n",
      "-> uber_trips_prod: index_trip 849 enviado. Total: 18\n",
      "-> uber_trips_clean: index_trip 849 enviado. Total: 18\n",
      "-> uber_trips_prod: index_trip 247 enviado. Total: 19\n",
      "-> uber_trips_clean: index_trip 247 enviado. Total: 19\n",
      "-> uber_trips_prod: index_trip 796 enviado. Total: 20\n",
      "-> uber_trips_clean: index_trip 796 enviado. Total: 20\n",
      "-> uber_trips_clean: index_trip 919 enviado. Total: 21\n",
      "-> uber_trips_prod: index_trip 919 enviado. Total: 21\n",
      "-> uber_trips_prod: index_trip 214 enviado. Total: 22-> uber_trips_clean: index_trip 214 enviado. Total: 22\n",
      "\n",
      "-> uber_trips_prod: index_trip 429 enviado. Total: 23\n",
      "-> uber_trips_clean: index_trip 429 enviado. Total: 23\n",
      "-> uber_trips_prod: index_trip 117 enviado. Total: 24\n",
      "-> uber_trips_clean: index_trip 117 enviado. Total: 24\n",
      "-> uber_trips_prod: index_trip 998 enviado. Total: 25\n",
      "-> uber_trips_clean: index_trip 998 enviado. Total: 25\n",
      "-> uber_trips_prod: index_trip 547 enviado. Total: 26\n",
      "-> uber_trips_clean: index_trip 547 enviado. Total: 26\n",
      "-> uber_trips_prod: index_trip 144 enviado. Total: 27\n",
      "-> uber_trips_clean: index_trip 144 enviado. Total: 27\n",
      "-> uber_trips_prod: index_trip 948 enviado. Total: 28\n",
      "-> uber_trips_clean: index_trip 948 enviado. Total: 28\n",
      "-> uber_trips_prod: index_trip 718 enviado. Total: 29\n",
      "-> uber_trips_clean: index_trip 718 enviado. Total: 29\n",
      "-> uber_trips_prod: index_trip 567 enviado. Total: 30-> uber_trips_clean: index_trip 567 enviado. Total: 30\n",
      "\n",
      "-> uber_trips_prod: index_trip 120 enviado. Total: 31\n",
      "-> uber_trips_clean: index_trip 120 enviado. Total: 31\n",
      "-> uber_trips_prod: index_trip 504 enviado. Total: 32\n",
      "-> uber_trips_clean: index_trip 504 enviado. Total: 32\n",
      "-> uber_trips_prod: index_trip 729 enviado. Total: 33-> uber_trips_clean: index_trip 729 enviado. Total: 33\n",
      "\n",
      "-> uber_trips_clean: index_trip 219 enviado. Total: 34-> uber_trips_prod: index_trip 219 enviado. Total: 34\n",
      "\n",
      "-> uber_trips_prod: index_trip 695 enviado. Total: 35\n",
      "-> uber_trips_clean: index_trip 695 enviado. Total: 35\n",
      "-> uber_trips_prod: index_trip 475 enviado. Total: 36\n",
      "-> uber_trips_clean: index_trip 475 enviado. Total: 36\n",
      "-> uber_trips_prod: index_trip 140 enviado. Total: 37-> uber_trips_clean: index_trip 140 enviado. Total: 37\n",
      "\n",
      "-> uber_trips_prod: index_trip 308 enviado. Total: 38-> uber_trips_clean: index_trip 308 enviado. Total: 38\n",
      "\n",
      "-> uber_trips_clean: index_trip 782 enviado. Total: 39\n",
      "-> uber_trips_prod: index_trip 782 enviado. Total: 39\n",
      "-> uber_trips_clean: index_trip 860 enviado. Total: 40\n",
      "-> uber_trips_prod: index_trip 860 enviado. Total: 40\n",
      "-> uber_trips_clean: index_trip 483 enviado. Total: 41\n",
      "-> uber_trips_prod: index_trip 483 enviado. Total: 41\n",
      "-> uber_trips_clean: index_trip 55 enviado. Total: 42\n",
      "-> uber_trips_prod: index_trip 55 enviado. Total: 42\n",
      "-> uber_trips_clean: index_trip 441 enviado. Total: 43\n",
      "-> uber_trips_prod: index_trip 441 enviado. Total: 43\n",
      "-> uber_trips_clean: index_trip 779 enviado. Total: 44\n",
      "-> uber_trips_prod: index_trip 779 enviado. Total: 44\n",
      "-> uber_trips_clean: index_trip 976 enviado. Total: 45\n",
      "-> uber_trips_prod: index_trip 976 enviado. Total: 45\n",
      "-> uber_trips_clean: index_trip 662 enviado. Total: 46\n",
      "-> uber_trips_prod: index_trip 662 enviado. Total: 46\n",
      "-> uber_trips_clean: index_trip 255 enviado. Total: 47\n",
      "-> uber_trips_prod: index_trip 255 enviado. Total: 47\n",
      "-> uber_trips_prod: index_trip 331 enviado. Total: 48\n",
      "-> uber_trips_clean: index_trip 331 enviado. Total: 48\n",
      "-> uber_trips_prod: index_trip 129 enviado. Total: 49\n",
      "-> uber_trips_clean: index_trip 129 enviado. Total: 49\n",
      "-> uber_trips_prod: index_trip 530 enviado. Total: 50\n",
      "-> uber_trips_clean: index_trip 530 enviado. Total: 50\n",
      "Completado: uber_trips_prod: 50 filas leídas y enviadas.Completado: uber_trips_clean: 50 filas leídas y enviadas.\n",
      "\n",
      "\n",
      "Proceso de producción finalizado. Conexiones a Kafka y Cassandra cerradas.\n"
     ]
    }
   ],
   "source": [
    "from cassandra.cluster import Cluster\n",
    "from kafka import KafkaProducer\n",
    "import json\n",
    "import threading\n",
    "import time\n",
    "from datetime import datetime, timezone\n",
    "\n",
    "# =======================================\n",
    "# 1. CASSANDRA & KAFKA CONFIGURACIÓN\n",
    "# =======================================\n",
    "\n",
    "# --- AJUSTE CRÍTICO DE HOST ---\n",
    "# Si tu ETL de Spark usó 'localhost' y estás ejecutando este script en la misma máquina,\n",
    "# cambia '100.68.89.127' a 'localhost'.\n",
    "CASSANDRA_HOST = '100.68.89.127' \n",
    "CASSANDRA_PORT = '9042'\n",
    "KEYSPACE = 'trips'\n",
    "\n",
    "try:\n",
    "    cluster_src = Cluster([CASSANDRA_HOST], port=int(CASSANDRA_PORT))\n",
    "    session_src = cluster_src.connect(KEYSPACE)\n",
    "    print(f\"Conexión con Cassandra establecida en el host: {CASSANDRA_HOST}\")\n",
    "except Exception as e:\n",
    "    print(f\"ERROR CRÍTICO: No se pudo conectar a Cassandra en {CASSANDRA_HOST}:{CASSANDRA_PORT}.\")\n",
    "    print(\"Asegúrate de que el servicio esté corriendo y la IP sea correcta.\")\n",
    "    raise e\n",
    "\n",
    "\n",
    "# Configuración de Kafka (Destino de los eventos)\n",
    "KAFKA_BROKER = '100.68.89.127:9092' \n",
    "producer = KafkaProducer(\n",
    "    bootstrap_servers=KAFKA_BROKER,\n",
    "    # Serializa el diccionario de Python a JSON y luego a bytes (utf-8)\n",
    "    value_serializer=lambda m: json.dumps(m).encode('utf-8')\n",
    ")\n",
    "print(f\"Conexión a Kafka Broker establecida: {KAFKA_BROKER}\")\n",
    "\n",
    "# Definición de la lista de columnas para el SELECT (mejora la eficiencia en Cassandra)\n",
    "COLUMNAS_CASSANDRA_SELECT = \"\"\"\n",
    "    index_trip, request_datetime, pickup_datetime, dropoff_datetime, \n",
    "    pulocationid, dolocationid, base_passenger_fare, tolls, \n",
    "    bcf, sales_tax, congestion_surcharge, airport_fee, tips, \n",
    "    driver_pay, shared_request_flag, shared_match_flag, access_a_ride_flag,\n",
    "    wav_request_flag, wav_match_flag, on_time_pickup, hour, year, month, day, \n",
    "    uber_sales, pickup_zone, delivery_zone\n",
    "\"\"\"\n",
    "\n",
    "# =======================================\n",
    "# 2. PRODUCER 1 – uber_trips_clean\n",
    "# =======================================\n",
    "def produce_uber_trips_clean():\n",
    "    \"\"\"Lee datos de uber_trips_clean y los envía al topic 'uber_trips_clean'.\"\"\"\n",
    "    TOPIC = \"uber_trips_clean\"\n",
    "    TABLE = \"uber_trips_clean\"\n",
    "    print(f\"\\n-> Producer {TOPIC} iniciado\")\n",
    "    \n",
    "    # Lectura de la tabla de datos limpios: limitamos a 50 para simulación.\n",
    "    query = f\"SELECT {COLUMNAS_CASSANDRA_SELECT} FROM {TABLE} LIMIT 50\"\n",
    "    \n",
    "    try:\n",
    "        rows = session_src.execute(query)\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR al ejecutar consulta CQL en {TABLE}: {e}\")\n",
    "        return\n",
    "\n",
    "    has_data = False\n",
    "    count = 0\n",
    "    for row in rows:\n",
    "        has_data = True\n",
    "        count += 1\n",
    "        \n",
    "        msg = {\n",
    "            \"index_trip\": row.index_trip,\n",
    "            \"request_datetime\": row.request_datetime.replace(tzinfo=timezone.utc).isoformat(),\n",
    "            \"pickup_datetime\": row.pickup_datetime.replace(tzinfo=timezone.utc).isoformat(),\n",
    "            \"dropoff_datetime\": row.dropoff_datetime.replace(tzinfo=timezone.utc).isoformat(),\n",
    "            \"pulocationid\": row.pulocationid, \n",
    "            \"dolocationid\": row.dolocationid,\n",
    "            \"base_passenger_fare\": row.base_passenger_fare,\n",
    "            \"tips\": row.tips,\n",
    "            \"driver_pay\": row.driver_pay,\n",
    "            \"uber_sales\": row.uber_sales,\n",
    "            \"pickup_zone\": row.pickup_zone,\n",
    "            \"delivery_zone\": row.delivery_zone,\n",
    "            \"source\": TABLE,\n",
    "            \"sent_at\": datetime.now(timezone.utc).isoformat()\n",
    "        }\n",
    "\n",
    "        producer.send(TOPIC, msg)\n",
    "        print(f\"-> {TOPIC}: index_trip {row.index_trip} enviado. Total: {count}\")\n",
    "        time.sleep(5) # Simulación de envío\n",
    "\n",
    "    if not has_data:\n",
    "        print(f\"ADVERTENCIA: Tabla {TABLE} NO TIENE DATOS para enviar. (Revisa el ETL)\")\n",
    "    else:\n",
    "        print(f\"Completado: {TABLE}: {count} filas leídas y enviadas.\")\n",
    "\n",
    "# =======================================\n",
    "# 3. PRODUCER 2 – uber_trips_prod\n",
    "# =======================================\n",
    "def produce_uber_trips_prod():\n",
    "    \"\"\"Lee datos de uber_trips_prod y los envía al topic 'uber_trips_prod'.\"\"\"\n",
    "    TOPIC = \"uber_trips_prod\"\n",
    "    TABLE = \"uber_trips_prod\"\n",
    "    print(f\"\\n-> Producer {TOPIC} iniciado\")\n",
    "    \n",
    "    # Lectura de la tabla de producción: limitamos a 50 para simulación.\n",
    "    query = f\"SELECT {COLUMNAS_CASSANDRA_SELECT} FROM {TABLE} LIMIT 50\"\n",
    "    \n",
    "    try:\n",
    "        rows = session_src.execute(query)\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR al ejecutar consulta CQL en {TABLE}: {e}\")\n",
    "        return\n",
    "\n",
    "    has_data = False\n",
    "    count = 0\n",
    "    for row in rows:\n",
    "        has_data = True\n",
    "        count += 1\n",
    "        \n",
    "        msg = {\n",
    "            \"index_trip\": row.index_trip,\n",
    "            \"request_datetime\": row.request_datetime.replace(tzinfo=timezone.utc).isoformat(),\n",
    "            \"pickup_datetime\": row.pickup_datetime.replace(tzinfo=timezone.utc).isoformat(),\n",
    "            \"dropoff_datetime\": row.dropoff_datetime.replace(tzinfo=timezone.utc).isoformat(),\n",
    "            \"pulocationid\": row.pulocationid,\n",
    "            \"dolocationid\": row.dolocationid,\n",
    "            \"base_passenger_fare\": row.base_passenger_fare,\n",
    "            \"tips\": row.tips,\n",
    "            \"driver_pay\": row.driver_pay,\n",
    "            \"uber_sales\": row.uber_sales,\n",
    "            \"pickup_zone\": row.pickup_zone,\n",
    "            \"delivery_zone\": row.delivery_zone,\n",
    "            \"source\": TABLE,\n",
    "            \"sent_at\": datetime.now(timezone.utc).isoformat()\n",
    "        }\n",
    "\n",
    "        producer.send(TOPIC, msg)\n",
    "        print(f\"-> {TOPIC}: index_trip {row.index_trip} enviado. Total: {count}\")\n",
    "        time.sleep(5) \n",
    "\n",
    "    if not has_data:\n",
    "        print(f\"ADVERTENCIA: Tabla {TABLE} NO TIENE DATOS para enviar. (Revisa el ETL)\")\n",
    "    else:\n",
    "        print(f\"Completado: {TABLE}: {count} filas leídas y enviadas.\")\n",
    "\n",
    "\n",
    "# =======================================\n",
    "# 4. EJECUCIÓN SIMULTÁNEA Y CIERRE\n",
    "# =======================================\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    # Inicialización de hilos para ejecución concurrente\n",
    "    t1 = threading.Thread(target=produce_uber_trips_clean)\n",
    "    t2 = threading.Thread(target=produce_uber_trips_prod)\n",
    "\n",
    "    t1.start()\n",
    "    t2.start()\n",
    "\n",
    "    # Esperar a que ambos hilos terminen\n",
    "    t1.join()\n",
    "    t2.join()\n",
    "\n",
    "    # Asegurar que todos los mensajes pendientes sean enviados\n",
    "    producer.flush()\n",
    "    \n",
    "    # Cerrar conexiones\n",
    "    producer.close()\n",
    "    session_src.shutdown()\n",
    "    cluster_src.shutdown()\n",
    "\n",
    "    print(\"\\nProceso de producción finalizado. Conexiones a Kafka y Cassandra cerradas.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c77a801-6138-408b-bbd3-2c01018bbc45",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ebe3eae-0415-4156-9fe9-1837b1162717",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
