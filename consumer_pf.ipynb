{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1b993b7f-0355-4ba8-b0e4-a24042dad055",
   "metadata": {},
   "source": [
    "# CONSUMER"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cebfdce-371a-4c7b-8f74-58d76a84017a",
   "metadata": {},
   "source": [
    "#### 3.8 Consumo de datos desde Kafka\n",
    "\n",
    "En esta etapa se realiza el consumo de datos en tiempo real utilizando **Apache Kafka**. Para ello, se importan las librerías necesarias, incluyendo `KafkaConsumer` y PySpark. Posteriormente, se configura el consumidor estableciendo la conexión con el servidor de Kafka y se define el proceso de deserialización de los mensajes, permitiendo transformar los datos recibidos en un formato adecuado para su posterior procesamiento y análisis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc966f08-ee82-41ee-8633-58e4a9c88b3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from kafka import KafkaConsumer\n",
    "import json\n",
    "import time\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "from IPython.display import clear_output, display\n",
    "\n",
    "# =========================\n",
    "# Constantes de conexión\n",
    "# =========================\n",
    "CASSANDRA_HOST = \"100.68.89.127\"\n",
    "CASSANDRA_PORT = \"9042\"\n",
    "\n",
    "KAFKA_HOST = \"100.68.89.127\"\n",
    "KAFKA_PORT = \"9092\"\n",
    "KAFKA_TOPIC = \"uber_trips\"\n",
    "KAFKA_GROUP_ID = \"grupo_analisis\"\n",
    "\n",
    "# =========================\n",
    "# Configuración del Consumer\n",
    "# =========================\n",
    "consumer = KafkaConsumer(\n",
    "    KAFKA_TOPIC,\n",
    "    bootstrap_servers=f\"{KAFKA_HOST}:{KAFKA_PORT}\",\n",
    "    auto_offset_reset=\"earliest\",\n",
    "    group_id=KAFKA_GROUP_ID,\n",
    "    value_deserializer=lambda m: json.loads(m.decode(\"utf-8\")),\n",
    "    enable_auto_commit=True\n",
    ")\n",
    "\n",
    "print(\"Kafka Consumer inicializado correctamente\")\n",
    "print(consumer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf3266b6-ea2d-48d1-b4dd-fff3cd6eeaf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# =========================\n",
    "# Crear SparkSession\n",
    "# =========================\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"KafkaTripsConsumer\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# =========================\n",
    "# Cargar archivo de zonas\n",
    "# =========================\n",
    "loc_raw = spark.read.csv(\n",
    "    \"data/taxi_zone_lookup.csv\",\n",
    "    header=True,\n",
    "    inferSchema=True\n",
    ")\n",
    "\n",
    "# =========================\n",
    "# Limpieza de columnas\n",
    "# =========================\n",
    "loc = loc_raw.drop(\"Borough\", \"service_zone\")\n",
    "\n",
    "# =========================\n",
    "# Preparación de DataFrames para joins\n",
    "# =========================\n",
    "loc_pickup = loc.withColumnRenamed(\"Zone\", \"pickup_zone\")\n",
    "loc_delivery = loc.withColumnRenamed(\"Zone\", \"delivery_zone\")\n",
    "\n",
    "# =========================\n",
    "# Verificación\n",
    "# =========================\n",
    "loc_pickup.show(5)\n",
    "loc_pickup.printSchema()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10f92245-fa21-4aed-b7fa-9518a699a7ea",
   "metadata": {},
   "source": [
    "#### 3.9 Almacenamiento y procesamiento de datos\n",
    "\n",
    "En esta etapa se realiza el consumo de datos provenientes de **Apache Kafka** mediante el uso de `KafkaConsumer`. A partir de los mensajes recibidos, se construye un DataFrame que permite almacenar los datos y aplicar las transformaciones requeridas para el análisis.\n",
    "\n",
    "Los datos provenientes de Kafka se leen dentro de un bucle de consumo continuo y se almacenan temporalmente en una estructura denominada `data_batch`. Con el fin de optimizar el procesamiento, se limita el número de registros por lote, y mediante una lógica condicional se transforma cada lote de datos en un DataFrame de Spark sobre el cual se aplican las siguientes operaciones:\n",
    "\n",
    "- Limpieza de registros con valores nulos.\n",
    "- Unión (*join*) con la tabla de ubicaciones utilizando los identificadores de origen y destino de los viajes.\n",
    "- Cálculo de las ganancias de la plataforma por cada viaje.\n",
    "- Cálculo del acumulado de las ganancias generadas por la plataforma.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a537243-34ba-4ada-aa5d-2f0dbe5c51bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import (\n",
    "    StructType, StructField,\n",
    "    IntegerType, StringType, FloatType\n",
    ")\n",
    "from pyspark.sql.functions import col\n",
    "from datetime import datetime\n",
    "\n",
    "# =========================\n",
    "# Esquema de los datos Kafka\n",
    "# =========================\n",
    "schema = StructType([\n",
    "    StructField(\"index_trip\", IntegerType(), True),\n",
    "    StructField(\"request_datetime\", StringType(), True),\n",
    "    StructField(\"pickup_datetime\", StringType(), True),\n",
    "    StructField(\"dropoff_datetime\", StringType(), True),\n",
    "    StructField(\"PULocationID\", IntegerType(), True),\n",
    "    StructField(\"DOLocationID\", IntegerType(), True),\n",
    "    StructField(\"base_passenger_fare\", FloatType(), True),\n",
    "    StructField(\"tips\", FloatType(), True),\n",
    "    StructField(\"driver_pay\", FloatType(), True),\n",
    "    StructField(\"hour\", FloatType(), True),\n",
    "    StructField(\"year\", FloatType(), True),\n",
    "    StructField(\"month\", FloatType(), True),\n",
    "    StructField(\"day\", FloatType(), True),\n",
    "    StructField(\"on_time_pickup\", IntegerType(), True)\n",
    "])\n",
    "\n",
    "# =========================\n",
    "# Configuración de batch\n",
    "# =========================\n",
    "data_batch = []\n",
    "BATCH_SIZE = 1   # Ajustable según volumen y recursos\n",
    "\n",
    "# =========================\n",
    "# Consumo de mensajes Kafka\n",
    "# =========================\n",
    "for message in consumer:\n",
    "    record = message.value\n",
    "    data_batch.append(record)\n",
    "\n",
    "    print(record)\n",
    "\n",
    "    # =========================\n",
    "    # Procesar lote\n",
    "    # =========================\n",
    "    if len(data_batch) >= BATCH_SIZE:\n",
    "        print(\"✔ Lote completo. Procesando datos...\")\n",
    "\n",
    "        # Crear DataFrame Spark\n",
    "        df = spark.createDataFrame(data_batch, schema)\n",
    "\n",
    "        # Limpieza de registros nulos\n",
    "        df_clean = df.dropna()\n",
    "\n",
    "        # Cálculo de ganancias de la plataforma\n",
    "        df_sales = df_clean.withColumn(\n",
    "            \"uber_sales\",\n",
    "            col(\"base_passenger_fare\") + col(\"tips\") - col(\"driver_pay\")\n",
    "        )\n",
    "\n",
    "        # Join con tabla de zonas de inicio\n",
    "        df_pickup = df_sales.join(\n",
    "            loc_1,\n",
    "            df_sales.PULocationID == loc_1.LocationID,\n",
    "            how=\"left\"\n",
    "        ).drop(\"LocationID\")\n",
    "\n",
    "        # Join con tabla de zonas de destino\n",
    "        df_final = df_pickup.join(\n",
    "            loc_2,\n",
    "            df_pickup.DOLocationID == loc_2.LocationID,\n",
    "            how=\"left\"\n",
    "        ).drop(\"LocationID\")\n",
    "\n",
    "        # Agregación de ganancias por fecha y hora\n",
    "        df_agg = df_final.groupBy(\n",
    "            \"year\", \"month\", \"day\", \"hour\"\n",
    "        ).sum(\"uber_sales\")\n",
    "\n",
    "        # =========================\n",
    "        # Persistencia de resultados\n",
    "        # =========================\n",
    "        df_final.write \\\n",
    "            .mode(\"append\") \\\n",
    "            .option(\"header\", True) \\\n",
    "            .csv(\"Proyecto_Final/clean_data.csv\")\n",
    "\n",
    "        # df_agg.write \\\n",
    "        #     .mode(\"append\") \\\n",
    "        #     .option(\"header\", True) \\\n",
    "        #     .csv(\"Proyecto_Final/aggregated_data.csv\")\n",
    "\n",
    "        # Limpiar batch\n",
    "        data_batch.clear()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db14e502-aceb-4d43-ade9-90549708ba59",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged4.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "095cf3e1-3159-469d-bb03-e8f3c3dc4886",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged4.explain()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
