{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ffafeb40-1ca4-48f5-8d77-7d5defd9ba81",
   "metadata": {},
   "source": [
    "#### Consumo de datos desde Kafka\n",
    "\n",
    "En esta etapa se realiza el consumo de datos en tiempo real utilizando **Apache Kafka**. Para ello, se importan las librer√≠as necesarias, incluyendo `KafkaConsumer` y PySpark. Posteriormente, se configura el consumidor estableciendo la conexi√≥n con el servidor de Kafka y se define el proceso de deserializaci√≥n de los mensajes, permitiendo transformar los datos recibidos en un formato adecuado para su posterior procesamiento y an√°lisis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96633409-d717-4b9b-9321-3cc652eba105",
   "metadata": {},
   "source": [
    "#### Almacenamiento y procesamiento de datos\n",
    "\n",
    "En esta etapa se realiza el consumo de datos provenientes de **Apache Kafka** mediante el uso de `KafkaConsumer`. A partir de los mensajes recibidos, se construye un DataFrame que permite almacenar los datos y aplicar las transformaciones requeridas para el an√°lisis.\n",
    "\n",
    "Los datos provenientes de Kafka se leen dentro de un bucle de consumo continuo y se almacenan temporalmente en una estructura denominada `data_batch`. Con el fin de optimizar el procesamiento, se limita el n√∫mero de registros por lote, y mediante una l√≥gica condicional se transforma cada lote de datos en un DataFrame de Spark sobre el cual se aplican las siguientes operaciones:\n",
    "\n",
    "- Limpieza de registros con valores nulos.\n",
    "- Uni√≥n (*join*) con la tabla de ubicaciones utilizando los identificadores de origen y destino de los viajes.\n",
    "- C√°lculo de las ganancias de la plataforma por cada viaje.\n",
    "- C√°lculo del acumulado de las ganancias generadas por la plataforma.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5c6449d-6739-4bac-924b-e89b17f5056f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Using incubator modules: jdk.incubator.vector\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/12/16 00:16:11 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession iniciada para procesamiento.\n",
      "Consumidor de Kafka suscrito a: ['uber_trips_clean', 'uber_trips_prod']\n",
      "\n",
      "--- INICIANDO CONSUMO Y PROCESAMIENTO EN TIEMPO REAL ---\n",
      "Recibido mensaje de uber_trips_prod: Trip ID 769\n",
      "Recibido mensaje de uber_trips_prod: Trip ID 23\n",
      "Recibido mensaje de uber_trips_prod: Trip ID 114\n",
      "Recibido mensaje de uber_trips_clean: Trip ID 769\n",
      "Recibido mensaje de uber_trips_clean: Trip ID 23\n",
      "Recibido mensaje de uber_trips_clean: Trip ID 114\n",
      "Recibido mensaje de uber_trips_clean: Trip ID 660\n",
      "Recibido mensaje de uber_trips_prod: Trip ID 660\n",
      "Recibido mensaje de uber_trips_clean: Trip ID 893\n",
      "Recibido mensaje de uber_trips_prod: Trip ID 893\n",
      "\n",
      "[Batch Completo - 00:16:24] Procesando 10 registros con Spark.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/12/16 00:16:29 WARN BasicWriteTaskStatsTracker: Expected 1 files, but only saw 0. This could be due to the output format not writing empty files, or files being not immediately visible in the filesystem.\n",
      "25/12/16 00:16:29 WARN BasicWriteTaskStatsTracker: Expected 1 files, but only saw 0. This could be due to the output format not writing empty files, or files being not immediately visible in the filesystem.\n",
      "25/12/16 00:16:29 WARN BasicWriteTaskStatsTracker: Expected 1 files, but only saw 0. This could be due to the output format not writing empty files, or files being not immediately visible in the filesystem.\n",
      "25/12/16 00:16:29 WARN BasicWriteTaskStatsTracker: Expected 1 files, but only saw 0. This could be due to the output format not writing empty files, or files being not immediately visible in the filesystem.\n",
      "25/12/16 00:16:29 WARN BasicWriteTaskStatsTracker: Expected 1 files, but only saw 0. This could be due to the output format not writing empty files, or files being not immediately visible in the filesystem.\n",
      "25/12/16 00:16:29 WARN BasicWriteTaskStatsTracker: Expected 1 files, but only saw 0. This could be due to the output format not writing empty files, or files being not immediately visible in the filesystem.\n",
      "25/12/16 00:16:29 WARN BasicWriteTaskStatsTracker: Expected 1 files, but only saw 0. This could be due to the output format not writing empty files, or files being not immediately visible in the filesystem.\n",
      "25/12/16 00:16:29 WARN BasicWriteTaskStatsTracker: Expected 1 files, but only saw 0. This could be due to the output format not writing empty files, or files being not immediately visible in the filesystem.\n",
      "25/12/16 00:16:29 WARN BasicWriteTaskStatsTracker: Expected 1 files, but only saw 0. This could be due to the output format not writing empty files, or files being not immediately visible in the filesystem.\n",
      "25/12/16 00:16:29 WARN BasicWriteTaskStatsTracker: Expected 1 files, but only saw 0. This could be due to the output format not writing empty files, or files being not immediately visible in the filesystem.\n",
      "25/12/16 00:16:30 WARN BasicWriteTaskStatsTracker: Expected 1 files, but only saw 0. This could be due to the output format not writing empty files, or files being not immediately visible in the filesystem.\n",
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-> Guardados 10 registros limpios y agregados.\n",
      "\n",
      "--- Top Zonas con Mayores Ventas en este Batch ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-------------------+----+\n",
      "|pickup_zone        |total_sales_zone   |hour|\n",
      "+-------------------+-------------------+----+\n",
      "|Murray Hill        |-0.5799999237060547|NULL|\n",
      "|Kingsbridge Heights|3.010000228881836  |NULL|\n",
      "|Bath Beach         |-0.1400003433227539|NULL|\n",
      "+-------------------+-------------------+----+\n",
      "\n",
      "---------------------------------------------------\n",
      "Recibido mensaje de uber_trips_clean: Trip ID 53\n",
      "Recibido mensaje de uber_trips_clean: Trip ID 987\n",
      "Recibido mensaje de uber_trips_prod: Trip ID 53\n",
      "Recibido mensaje de uber_trips_prod: Trip ID 987\n",
      "Recibido mensaje de uber_trips_clean: Trip ID 878\n",
      "Recibido mensaje de uber_trips_prod: Trip ID 878\n",
      "Recibido mensaje de uber_trips_clean: Trip ID 110\n",
      "Recibido mensaje de uber_trips_prod: Trip ID 110\n",
      "Recibido mensaje de uber_trips_clean: Trip ID 91\n",
      "Recibido mensaje de uber_trips_prod: Trip ID 91\n",
      "\n",
      "[Batch Completo - 00:16:49] Procesando 10 registros con Spark.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-> Guardados 10 registros limpios y agregados.\n",
      "\n",
      "--- Top Zonas con Mayores Ventas en este Batch ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------+------------------+----+\n",
      "|pickup_zone              |total_sales_zone  |hour|\n",
      "+-------------------------+------------------+----+\n",
      "|Williamsbridge/Olinville |2.5799999237060547|NULL|\n",
      "|Springfield Gardens North|5.029999732971191 |NULL|\n",
      "|Williamsbridge/Olinville |2.5799999237060547|NULL|\n",
      "+-------------------------+------------------+----+\n",
      "\n",
      "---------------------------------------------------\n",
      "Recibido mensaje de uber_trips_clean: Trip ID 128\n",
      "Recibido mensaje de uber_trips_prod: Trip ID 128\n",
      "Recibido mensaje de uber_trips_clean: Trip ID 363\n",
      "Recibido mensaje de uber_trips_prod: Trip ID 363\n",
      "Recibido mensaje de uber_trips_clean: Trip ID 251\n",
      "Recibido mensaje de uber_trips_prod: Trip ID 251\n",
      "Recibido mensaje de uber_trips_clean: Trip ID 744\n",
      "Recibido mensaje de uber_trips_prod: Trip ID 744\n",
      "Recibido mensaje de uber_trips_prod: Trip ID 778\n",
      "Recibido mensaje de uber_trips_clean: Trip ID 778\n",
      "\n",
      "[Batch Completo - 00:17:14] Procesando 10 registros con Spark.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-> Guardados 10 registros limpios y agregados.\n",
      "\n",
      "--- Top Zonas con Mayores Ventas en este Batch ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------+------------------+----+\n",
      "|pickup_zone              |total_sales_zone  |hour|\n",
      "+-------------------------+------------------+----+\n",
      "|Kips Bay                 |5.970000267028809 |NULL|\n",
      "|Kips Bay                 |5.970000267028809 |NULL|\n",
      "|Williamsburg (North Side)|1.3400006294250488|NULL|\n",
      "+-------------------------+------------------+----+\n",
      "\n",
      "---------------------------------------------------\n",
      "Recibido mensaje de uber_trips_prod: Trip ID 819\n",
      "Recibido mensaje de uber_trips_clean: Trip ID 819\n",
      "Recibido mensaje de uber_trips_prod: Trip ID 310\n",
      "Recibido mensaje de uber_trips_clean: Trip ID 310\n",
      "Recibido mensaje de uber_trips_prod: Trip ID 849\n",
      "Recibido mensaje de uber_trips_clean: Trip ID 849\n",
      "Recibido mensaje de uber_trips_prod: Trip ID 247\n",
      "Recibido mensaje de uber_trips_clean: Trip ID 247\n",
      "Recibido mensaje de uber_trips_prod: Trip ID 796\n",
      "Recibido mensaje de uber_trips_clean: Trip ID 796\n",
      "\n",
      "[Batch Completo - 00:17:39] Procesando 10 registros con Spark.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-> Guardados 10 registros limpios y agregados.\n",
      "\n",
      "--- Top Zonas con Mayores Ventas en este Batch ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+-------------------+----+\n",
      "|pickup_zone     |total_sales_zone   |hour|\n",
      "+----------------+-------------------+----+\n",
      "|Brownsville     |-1.6899986267089844|NULL|\n",
      "|Brownsville     |-1.6899986267089844|NULL|\n",
      "|Manhattan Valley|10.029998779296875 |NULL|\n",
      "+----------------+-------------------+----+\n",
      "\n",
      "---------------------------------------------------\n",
      "Recibido mensaje de uber_trips_prod: Trip ID 919\n",
      "Recibido mensaje de uber_trips_clean: Trip ID 919\n",
      "Recibido mensaje de uber_trips_clean: Trip ID 214\n",
      "Recibido mensaje de uber_trips_prod: Trip ID 214\n",
      "Recibido mensaje de uber_trips_clean: Trip ID 429\n",
      "Recibido mensaje de uber_trips_prod: Trip ID 429\n",
      "Recibido mensaje de uber_trips_clean: Trip ID 117\n",
      "Recibido mensaje de uber_trips_prod: Trip ID 117\n",
      "Recibido mensaje de uber_trips_clean: Trip ID 998\n",
      "Recibido mensaje de uber_trips_prod: Trip ID 998\n",
      "\n",
      "[Batch Completo - 00:18:04] Procesando 10 registros con Spark.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-> Guardados 10 registros limpios y agregados.\n",
      "\n",
      "--- Top Zonas con Mayores Ventas en este Batch ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+------------------+----+\n",
      "|pickup_zone   |total_sales_zone  |hour|\n",
      "+--------------+------------------+----+\n",
      "|East New York |1.8499999046325684|NULL|\n",
      "|East New York |1.8499999046325684|NULL|\n",
      "|Manhattanville|2.9700002670288086|NULL|\n",
      "+--------------+------------------+----+\n",
      "\n",
      "---------------------------------------------------\n",
      "Recibido mensaje de uber_trips_clean: Trip ID 547\n",
      "Recibido mensaje de uber_trips_prod: Trip ID 547\n",
      "Recibido mensaje de uber_trips_clean: Trip ID 144\n",
      "Recibido mensaje de uber_trips_prod: Trip ID 144\n"
     ]
    }
   ],
   "source": [
    "from kafka import KafkaConsumer\n",
    "import json\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import (\n",
    "    StructType, StructField,\n",
    "    IntegerType, StringType, FloatType\n",
    ")\n",
    "from pyspark.sql.functions import col, sum, first\n",
    "import time\n",
    "from datetime import datetime\n",
    "import os\n",
    "\n",
    "# =====================================================\n",
    "# 1. CONFIGURACI√ìN GENERAL\n",
    "# =====================================================\n",
    "\n",
    "KAFKA_BROKER = '100.68.89.127:9092'\n",
    "TOPICS = ['uber_trips_clean', 'uber_trips_prod']\n",
    "CONSUMER_GROUP = 'uber_realtime_analysis'\n",
    "SIZE_BATCH = 10\n",
    "\n",
    "REALTIME_OUTPUT_DIR = \"output_realtime_analysis\"\n",
    "CLEAN_DIR = f\"{REALTIME_OUTPUT_DIR}/clean_data\"\n",
    "AGG_DIR = f\"{REALTIME_OUTPUT_DIR}/aggregated_data\"\n",
    "\n",
    "os.makedirs(CLEAN_DIR, exist_ok=True)\n",
    "os.makedirs(AGG_DIR, exist_ok=True)\n",
    "\n",
    "# =====================================================\n",
    "# 2. ESQUEMA DE DATOS\n",
    "# =====================================================\n",
    "\n",
    "ENRICHED_SCHEMA = StructType([\n",
    "    StructField(\"index_trip\", IntegerType(), True),\n",
    "    StructField(\"request_datetime\", StringType(), True),\n",
    "    StructField(\"pickup_datetime\", StringType(), True),\n",
    "    StructField(\"dropoff_datetime\", StringType(), True),\n",
    "    StructField(\"pulocationid\", IntegerType(), True),\n",
    "    StructField(\"dolocationid\", IntegerType(), True),\n",
    "    StructField(\"base_passenger_fare\", FloatType(), True),\n",
    "    StructField(\"tips\", FloatType(), True),\n",
    "    StructField(\"driver_pay\", FloatType(), True),\n",
    "    StructField(\"hour\", FloatType(), True),\n",
    "    StructField(\"year\", FloatType(), True),\n",
    "    StructField(\"month\", FloatType(), True),\n",
    "    StructField(\"day\", FloatType(), True),\n",
    "    StructField(\"on_time_pickup\", IntegerType(), True),\n",
    "\n",
    "    # Enriquecidas\n",
    "    StructField(\"uber_sales\", FloatType(), True),\n",
    "    StructField(\"pickup_zone\", StringType(), True),\n",
    "    StructField(\"delivery_zone\", StringType(), True),\n",
    "    StructField(\"source\", StringType(), True),\n",
    "    StructField(\"sent_at\", StringType(), True),\n",
    "])\n",
    "\n",
    "# =====================================================\n",
    "# 3. INICIALIZACI√ìN SPARK Y KAFKA\n",
    "# =====================================================\n",
    "\n",
    "spark = (\n",
    "    SparkSession.builder\n",
    "    .appName(\"KafkaTripsRealtimeProcessor\")\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "print(\"‚úÖ SparkSession iniciada\")\n",
    "\n",
    "consumer = KafkaConsumer(\n",
    "    *TOPICS,\n",
    "    bootstrap_servers=KAFKA_BROKER,\n",
    "    auto_offset_reset='earliest',\n",
    "    group_id=CONSUMER_GROUP,\n",
    "    value_deserializer=lambda m: json.loads(m.decode('utf-8')),\n",
    "    consumer_timeout_ms=10000\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ Consumidor Kafka suscrito a: {TOPICS}\")\n",
    "print(\"\\nüöÄ INICIANDO CONSUMO EN TIEMPO REAL\\n\")\n",
    "\n",
    "# =====================================================\n",
    "# 4. BUCLE PRINCIPAL\n",
    "# =====================================================\n",
    "\n",
    "data_batch = []\n",
    "\n",
    "try:\n",
    "    for message in consumer:\n",
    "\n",
    "        if message.value is None or not isinstance(message.value, dict):\n",
    "            continue\n",
    "\n",
    "        data_batch.append(message.value)\n",
    "\n",
    "        print(\n",
    "            f\"üì© Mensaje recibido [{message.topic}] \"\n",
    "            f\"Trip ID: {message.value.get('index_trip', 'N/A')}\"\n",
    "        )\n",
    "\n",
    "        # -------------------------------------------------\n",
    "        # PROCESAR BATCH\n",
    "        # -------------------------------------------------\n",
    "        if len(data_batch) >= SIZE_BATCH:\n",
    "\n",
    "            print(\n",
    "                f\"\\nüì¶ Batch completo \"\n",
    "                f\"({len(data_batch)} registros) \"\n",
    "                f\"{datetime.now().strftime('%H:%M:%S')}\"\n",
    "            )\n",
    "\n",
    "            df = spark.createDataFrame(data_batch, ENRICHED_SCHEMA)\n",
    "\n",
    "            # ------------------------------\n",
    "            # AGREGACI√ìN\n",
    "            # ------------------------------\n",
    "            df_agg_zone = (\n",
    "                df.groupBy(\"pickup_zone\", \"source\")\n",
    "                .agg(\n",
    "                    sum(\"uber_sales\").alias(\"total_sales_zone\"),\n",
    "                    first(\"year\").alias(\"year\"),\n",
    "                    first(\"hour\").alias(\"hour\")\n",
    "                )\n",
    "                .filter(col(\"total_sales_zone\") > 0)\n",
    "                .orderBy(col(\"total_sales_zone\").desc())\n",
    "            )\n",
    "\n",
    "            # ------------------------------\n",
    "            # ESCRITURA CLEAN DATA\n",
    "            # ------------------------------\n",
    "            df.coalesce(1).write.mode(\"append\").option(\n",
    "                \"header\", True\n",
    "            ).csv(CLEAN_DIR)\n",
    "\n",
    "            # ------------------------------\n",
    "            # ESCRITURA AGREGADOS (SOLO SI HAY DATOS)\n",
    "            # ------------------------------\n",
    "            if df_agg_zone.count() > 0:\n",
    "                df_agg_zone.coalesce(1).write.mode(\"append\").option(\n",
    "                    \"header\", True\n",
    "                ).csv(AGG_DIR)\n",
    "\n",
    "                print(\"‚úÖ CSV agregado escrito correctamente\")\n",
    "                print(f\"üìÇ Ruta: {os.path.abspath(AGG_DIR)}\")\n",
    "            else:\n",
    "                print(\"‚ö†Ô∏è Batch sin datos agregables ‚Äî no se escribe CSV\")\n",
    "\n",
    "            # ------------------------------\n",
    "            # DEBUG VISUAL\n",
    "            # ------------------------------\n",
    "            print(\"\\nüîù Top Zonas por Ventas (Batch)\")\n",
    "            df_agg_zone.select(\n",
    "                \"pickup_zone\", \"total_sales_zone\", \"hour\"\n",
    "            ).limit(3).show(truncate=False)\n",
    "\n",
    "            print(\"-\" * 50)\n",
    "\n",
    "            data_batch.clear()\n",
    "            time.sleep(1)\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå ERROR CR√çTICO EN CONSUMER: {e}\")\n",
    "\n",
    "finally:\n",
    "    consumer.close()\n",
    "    spark.stop()\n",
    "    print(\"\\nüõë Consumer detenido correctamente\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3a8e7bb-88e2-4741-81c3-926f0e2c5378",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
