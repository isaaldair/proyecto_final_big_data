{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "35944301-f82f-44d5-a257-9354728be2dc",
   "metadata": {},
   "source": [
    "# PRODUCER"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d44c354d-f46d-4af9-b16b-b6fd6a1b6760",
   "metadata": {},
   "source": [
    "# Análisis de Transacciones en la Plataforma Uber\n",
    "\n",
    "## Universidad Tecnológica de Panamá  \n",
    "### Facultad de Ingeniería de Sistemas Computacionales  \n",
    "### Maestría en Analítica de Datos  \n",
    "\n",
    "**Integrantes:**\n",
    "- Ávila, Isaac  \n",
    "- Sanjur, Andy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24bf0e60-72a3-4dfd-9a24-b92617856d92",
   "metadata": {},
   "source": [
    "### 1. Descripción del caso simulado\n",
    "\n",
    "En la última década, las plataformas de solicitud de viajes han adquirido una relevancia significativa en el ámbito de la movilidad urbana. Gracias a su practicidad, cada vez más usuarios y conductores optan por este tipo de soluciones de transporte privado. En una ciudad tan grande como Nueva York (NYC), pueden registrarse cientos de viajes por minuto, lo cual representa un reto importante para el análisis de datos en tiempo real mediante métodos convencionales, o bien vuelve ineficiente el análisis de datos históricos para el ajuste dinámico de tarifas según la demanda del momento.\n",
    "\n",
    "El siguiente caso simulado presenta una serie de transacciones realizadas por la plataforma Uber. Cada registro corresponde a un viaje efectuado desde un punto A hasta un punto B, incluyendo su respectivo detalle. A partir de estos datos, se busca analizar los siguientes aspectos:\n",
    "\n",
    "- ¿Cuáles son los puntos de partida más comunes?\n",
    "- ¿Cuáles son los destinos más concurridos?\n",
    "- ¿En qué días y a qué horas existe mayor frecuencia de solicitudes de viaje?\n",
    "- Determinar cuáles son las matrículas con mayor cantidad de demoras.\n",
    "- Determinar los minutos promedio de demora.\n",
    "\n",
    "Para el análisis se utiliza un conjunto de datos en formato `.csv`, recopilado de la plataforma Kaggle:  \n",
    "https://www.kaggle.com/datasets/ahmedramadan74/uber-nyc  \n",
    "\n",
    "Este conjunto de datos incluye los siguientes campos relevantes para el caso de uso:\n",
    "\n",
    "- **index_trip (int):** Número único del evento.\n",
    "- **hvfhs_license_num (str):** Tipo de servicio contratado.\n",
    "- **request_datetime (timestamp):** Fecha y hora en que se solicita el servicio.\n",
    "- **pickup_datetime (timestamp):** Fecha y hora de inicio del viaje.\n",
    "- **dropoff_datetime (timestamp):** Fecha y hora de finalización del viaje.\n",
    "- **PULocationID (int):** Zona donde inicia el viaje.\n",
    "- **DOLocationID (int):** Zona donde finaliza el viaje.\n",
    "- **trip_miles (double):** Distancia total del viaje.\n",
    "- **trip_time (int):** Duración del viaje en segundos.\n",
    "- **base_passenger_fare (double):** Tarifa base del viaje.\n",
    "- **tolls (double):** Total de peajes.\n",
    "- **sales_tax (double):** Total de impuestos.\n",
    "- **congestion_surcharge (double):** Costos adicionales por congestión vehicular.\n",
    "- **airport_fee (double):** Tarifa adicional por aeropuerto (USD 2.50).\n",
    "- **tips (double):** Propinas.\n",
    "- **driver_pay (double):** Pago al conductor.\n",
    "- **hour (double):** Hora de la solicitud.\n",
    "- **year (double):** Año de la solicitud.\n",
    "- **month (double):** Mes de la solicitud.\n",
    "- **day (double):** Día de la solicitud.\n",
    "- **on_time_pickup (int):** Indicador de llegada a tiempo.\n",
    "\n",
    "---\n",
    "\n",
    "### 2. Justificación del uso de Big Data en el problema\n",
    "\n",
    "La alta demanda y el gran volumen de información que puede generar la aplicación durante las horas pico dificultan el análisis en tiempo real mediante herramientas tradicionales de analítica de datos. En este contexto, las oportunidades de negocio pueden perder relevancia si la información no es procesada y utilizada de manera inmediata.\n",
    "\n",
    "Para este caso, con el objetivo de mantener la rapidez en el análisis y manejar eficientemente el volumen de datos que se generan cada minuto, se emplean tecnologías orientadas a Big Data, tales como **Apache Cassandra** para el almacenamiento distribuido de datos, **Apache Kafka** para la transmisión de datos en tiempo real (streaming) y **PySpark** para la transformación, consulta y exportación de la información analizada.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12b27a6d-94ff-47db-af3c-69424e094743",
   "metadata": {},
   "source": [
    "### 3. Proceso utilizado para el análisis\n",
    "\n",
    "#### 3.1 Creación de la sesión de Spark\n",
    "\n",
    "En esta etapa se importa la librería **SparkSession** y se crea la sesión de Apache Spark.  \n",
    "Para ello, se configura el **host** y el **puerto** de conexión que permitirán establecer la comunicación con el entorno de procesamiento distribuido.\n",
    "\n",
    "##### 3.1.1 Definición de constantes de configuración\n",
    "\n",
    "Con el fin de mejorar la mantenibilidad y portabilidad del código, se definen constantes que agrupan los parámetros de configuración de la sesión de Spark y la conexión con la base de datos Apache Cassandra. Este enfoque permite modificar fácilmente los valores de conexión sin afectar la lógica del procesamiento del análisis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9f9fceb3-6376-46b2-bdbf-d34038efa5f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CONSTANTES\n",
    "CASSANDRA_HOST = \"100.68.89.127\"\n",
    "CASSANDRA_PORT = \"9042\"\n",
    "\n",
    "APP_NAME = \"Proyecto_Final_Uber_Trips\"\n",
    "CASSANDRA_CONNECTOR = \"com.datastax.spark:spark-cassandra-connector_2.12:3.3.0\"\n",
    "\n",
    "CASSANDRA_CONNECTOR = \"com.datastax.spark:spark-cassandra-connector_2.13:3.5.1\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1bf1eedc-00da-4919-98b5-8023e468f60e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Using incubator modules: jdk.incubator.vector\n",
      ":: loading settings :: url = jar:file:/home/admin/jupyter_venv/lib/python3.12/site-packages/pyspark/jars/ivy-2.5.3.jar!/org/apache/ivy/core/settings/ivysettings.xml\n",
      "Ivy Default Cache set to: /home/admin/.ivy2.5.2/cache\n",
      "The jars for the packages stored in: /home/admin/.ivy2.5.2/jars\n",
      "com.datastax.spark#spark-cassandra-connector_2.13 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-6fa5f95d-b223-42f8-9900-445c22d9b95c;1.0\n",
      "\tconfs: [default]\n",
      "\tfound com.datastax.spark#spark-cassandra-connector_2.13;3.5.1 in central\n",
      "\tfound com.datastax.spark#spark-cassandra-connector-driver_2.13;3.5.1 in central\n",
      "\tfound org.scala-lang.modules#scala-collection-compat_2.13;2.11.0 in central\n",
      "\tfound org.scala-lang.modules#scala-parallel-collections_2.13;1.0.4 in central\n",
      "\tfound org.apache.cassandra#java-driver-core-shaded;4.18.1 in central\n",
      "\tfound com.datastax.oss#native-protocol;1.5.1 in central\n",
      "\tfound com.datastax.oss#java-driver-shaded-guava;25.1-jre-graal-sub-1 in central\n",
      "\tfound com.typesafe#config;1.4.1 in central\n",
      "\tfound org.slf4j#slf4j-api;1.7.26 in central\n",
      "\tfound io.dropwizard.metrics#metrics-core;4.1.18 in central\n",
      "\tfound org.hdrhistogram#HdrHistogram;2.1.12 in central\n",
      "\tfound org.reactivestreams#reactive-streams;1.0.3 in central\n",
      "\tfound org.apache.cassandra#java-driver-mapper-runtime;4.18.1 in central\n",
      "\tfound org.apache.cassandra#java-driver-query-builder;4.18.1 in central\n",
      "\tfound org.apache.commons#commons-lang3;3.10 in central\n",
      "\tfound com.thoughtworks.paranamer#paranamer;2.8 in central\n",
      "\tfound org.scala-lang#scala-reflect;2.13.13 in central\n",
      ":: resolution report :: resolve 688ms :: artifacts dl 13ms\n",
      "\t:: modules in use:\n",
      "\tcom.datastax.oss#java-driver-shaded-guava;25.1-jre-graal-sub-1 from central in [default]\n",
      "\tcom.datastax.oss#native-protocol;1.5.1 from central in [default]\n",
      "\tcom.datastax.spark#spark-cassandra-connector-driver_2.13;3.5.1 from central in [default]\n",
      "\tcom.datastax.spark#spark-cassandra-connector_2.13;3.5.1 from central in [default]\n",
      "\tcom.thoughtworks.paranamer#paranamer;2.8 from central in [default]\n",
      "\tcom.typesafe#config;1.4.1 from central in [default]\n",
      "\tio.dropwizard.metrics#metrics-core;4.1.18 from central in [default]\n",
      "\torg.apache.cassandra#java-driver-core-shaded;4.18.1 from central in [default]\n",
      "\torg.apache.cassandra#java-driver-mapper-runtime;4.18.1 from central in [default]\n",
      "\torg.apache.cassandra#java-driver-query-builder;4.18.1 from central in [default]\n",
      "\torg.apache.commons#commons-lang3;3.10 from central in [default]\n",
      "\torg.hdrhistogram#HdrHistogram;2.1.12 from central in [default]\n",
      "\torg.reactivestreams#reactive-streams;1.0.3 from central in [default]\n",
      "\torg.scala-lang#scala-reflect;2.13.13 from central in [default]\n",
      "\torg.scala-lang.modules#scala-collection-compat_2.13;2.11.0 from central in [default]\n",
      "\torg.scala-lang.modules#scala-parallel-collections_2.13;1.0.4 from central in [default]\n",
      "\torg.slf4j#slf4j-api;1.7.26 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   17  |   0   |   0   |   0   ||   17  |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-6fa5f95d-b223-42f8-9900-445c22d9b95c\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 17 already retrieved (0kB/9ms)\n",
      "25/12/15 20:56:35 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Creación de SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(APP_NAME) \\\n",
    "    .config(\"spark.jars.packages\", CASSANDRA_CONNECTOR) \\\n",
    "    .config(\"spark.cassandra.connection.host\", CASSANDRA_HOST) \\\n",
    "    .config(\"spark.cassandra.connection.port\", CASSANDRA_PORT) \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1662acd-c534-4ae8-b223-a09a709b2ee9",
   "metadata": {},
   "source": [
    "#### 3.2 Lectura e importación del conjunto de datos\n",
    "\n",
    "En esta etapa se realiza la lectura del conjunto de datos en formato `.csv` utilizando PySpark. Posteriormente, se visualizan las primeras cinco filas del conjunto de registros con el fin de validar la correcta carga de la información. Adicionalmente, se emplea la función `printSchema()` para inspeccionar la estructura del esquema y los tipos de datos asociados a cada columna.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a4e9feab-3eb4-4851-ae0c-c31a7f2ea168",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/12/15 20:56:49 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------------+--------------------+--------------------+-------------------+-------------------+-------------------+-------------------+------------+------------+----------+---------+-------------------+-----+----+---------+--------------------+-----------+----+----------+-------------------+-----------------+------------------+----------------+--------------+----+------+-----+----+--------------+\n",
      "|index_trip|hvfhs_license_num|dispatching_base_num|originating_base_num|   request_datetime|  on_scene_datetime|    pickup_datetime|   dropoff_datetime|PULocationID|DOLocationID|trip_miles|trip_time|base_passenger_fare|tolls| bcf|sales_tax|congestion_surcharge|airport_fee|tips|driver_pay|shared_request_flag|shared_match_flag|access_a_ride_flag|wav_request_flag|wav_match_flag|hour|  year|month| day|on_time_pickup|\n",
      "+----------+-----------------+--------------------+--------------------+-------------------+-------------------+-------------------+-------------------+------------+------------+----------+---------+-------------------+-----+----+---------+--------------------+-----------+----+----------+-------------------+-----------------+------------------+----------------+--------------+----+------+-----+----+--------------+\n",
      "|         0|             Uber|              B02878|              B02878|2021-01-09 14:24:31|2021-01-09 14:25:24|2021-01-09 14:26:27|2021-01-09 14:42:20|          90|         229|      2.22|      953|              14.41|  0.0|0.43|     1.28|                2.75|       NULL| 0.0|     10.42|                  N|                N|                  |               N|             N|14.0|2021.0|  1.0| 9.0|             1|\n",
      "|         1|             Uber|              B02883|              B02883|2021-01-21 17:42:10|2021-01-21 17:45:33|2021-01-21 17:46:16|2021-01-21 17:53:24|         232|           4|      1.12|      428|               7.91|  0.0|0.24|      0.7|                2.75|       NULL| 0.0|       5.4|                  N|                N|                  |               N|             N|17.0|2021.0|  1.0|21.0|             1|\n",
      "|         2|             Uber|              B02889|              B02889|2021-01-19 18:40:49|2021-01-19 18:41:01|2021-01-19 18:43:01|2021-01-19 18:58:57|         198|          37|      2.75|      956|              10.77|  0.0|0.32|     0.96|                 0.0|       NULL| 0.0|     11.04|                  N|                N|                  |               N|             N|18.0|2021.0|  1.0|19.0|             1|\n",
      "|         3|             Uber|              B02875|              B02875|2021-01-30 10:14:07|2021-01-30 10:15:40|2021-01-30 10:17:12|2021-01-30 10:22:18|         100|         161|      1.36|      306|               7.91|  0.0|0.24|      0.7|                2.75|       NULL| 3.0|       5.4|                  N|                N|                  |               N|             N|10.0|2021.0|  1.0|30.0|             1|\n",
      "|         4|             Uber|              B02876|              B02876|2021-01-23 23:13:09|2021-01-23 23:18:26|2021-01-23 23:20:26|2021-01-23 23:33:41|         168|         262|      3.04|      795|              13.77|  0.0|0.41|     1.22|                2.75|       NULL| 0.0|      10.0|                  N|                N|                  |               N|             N|23.0|2021.0|  1.0|23.0|             0|\n",
      "+----------+-----------------+--------------------+--------------------+-------------------+-------------------+-------------------+-------------------+------------+------------+----------+---------+-------------------+-----+----+---------+--------------------+-----------+----+----------+-------------------+-----------------+------------------+----------------+--------------+----+------+-----+----+--------------+\n",
      "only showing top 5 rows\n",
      "Esquema utilizado en el conjunto de datos:\n",
      "root\n",
      " |-- index_trip: integer (nullable = true)\n",
      " |-- hvfhs_license_num: string (nullable = true)\n",
      " |-- dispatching_base_num: string (nullable = true)\n",
      " |-- originating_base_num: string (nullable = true)\n",
      " |-- request_datetime: timestamp (nullable = true)\n",
      " |-- on_scene_datetime: timestamp (nullable = true)\n",
      " |-- pickup_datetime: timestamp (nullable = true)\n",
      " |-- dropoff_datetime: timestamp (nullable = true)\n",
      " |-- PULocationID: integer (nullable = true)\n",
      " |-- DOLocationID: integer (nullable = true)\n",
      " |-- trip_miles: double (nullable = true)\n",
      " |-- trip_time: integer (nullable = true)\n",
      " |-- base_passenger_fare: double (nullable = true)\n",
      " |-- tolls: double (nullable = true)\n",
      " |-- bcf: double (nullable = true)\n",
      " |-- sales_tax: double (nullable = true)\n",
      " |-- congestion_surcharge: double (nullable = true)\n",
      " |-- airport_fee: double (nullable = true)\n",
      " |-- tips: double (nullable = true)\n",
      " |-- driver_pay: double (nullable = true)\n",
      " |-- shared_request_flag: string (nullable = true)\n",
      " |-- shared_match_flag: string (nullable = true)\n",
      " |-- access_a_ride_flag: string (nullable = true)\n",
      " |-- wav_request_flag: string (nullable = true)\n",
      " |-- wav_match_flag: string (nullable = true)\n",
      " |-- hour: double (nullable = true)\n",
      " |-- year: double (nullable = true)\n",
      " |-- month: double (nullable = true)\n",
      " |-- day: double (nullable = true)\n",
      " |-- on_time_pickup: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "raw_trips=spark.read.csv(\"data/sampling_trips.csv\", header=True, inferSchema=True)\n",
    "raw_trips.show(5)\n",
    "print('Esquema utilizado en el conjunto de datos:')\n",
    "raw_trips.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4b264d4-c260-4d89-ab24-fb4a6f9f4767",
   "metadata": {},
   "source": [
    "#### 3.3 Integración con Cassandra\n",
    "\n",
    "En esta etapa se realiza la creación y conexión al clúster de **Apache Cassandra**, el cual es utilizado como sistema de almacenamiento distribuido. Se define el **keyspace** denominado `trips` y se crea la tabla `uber_trips`, estructurada de acuerdo con las columnas del conjunto de datos analizado. Finalmente, se efectúa la escritura de la información en Cassandra, persistiendo los datos previamente cargados y procesados mediante Apache Spark.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f3eff9b6-c0c3-42f6-bd98-2caa9db15e80",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:cassandra.cluster:Downgrading core protocol version from 66 to 65 for 127.0.0.1:9042. To avoid this, it is best practice to explicitly set Cluster(protocol_version) to the version supported by your cluster. https://docs.datastax.com/en/developer/python-driver/latest/api/cassandra/cluster.html#cassandra.cluster.Cluster.protocol_version\n",
      "WARNING:cassandra.cluster:Downgrading core protocol version from 65 to 5 for 127.0.0.1:9042. To avoid this, it is best practice to explicitly set Cluster(protocol_version) to the version supported by your cluster. https://docs.datastax.com/en/developer/python-driver/latest/api/cassandra/cluster.html#cassandra.cluster.Cluster.protocol_version\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datos de la tabla 'uber_trips' eliminados correctamente.\n",
      "Keyspace y tabla verificados.\n",
      "La tabla ya contiene datos. No se realiza inserción.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 8:==================================>                   (279 + 14) / 435]"
     ]
    }
   ],
   "source": [
    "from cassandra.cluster import Cluster\n",
    "from cassandra.policies import RoundRobinPolicy\n",
    "\n",
    "KEYSPACE = \"trips\"\n",
    "TABLE = \"uber_trips\"\n",
    "REPLICATION_FACTOR = 1\n",
    "BATCH_SIZE = 10000\n",
    "\n",
    "# =========================\n",
    "# Conexión a Cassandra\n",
    "# =========================\n",
    "cluster = Cluster(\n",
    "    contact_points=[\"127.0.0.1\"],\n",
    "    port=CASSANDRA_PORT,\n",
    "    load_balancing_policy=RoundRobinPolicy()\n",
    ")\n",
    "\n",
    "session = cluster.connect()\n",
    "\n",
    "BORRAR_DATOS = True\n",
    "\n",
    "if BORRAR_DATOS:\n",
    "    session.execute(\"TRUNCATE trips.uber_trips;\")\n",
    "    print(\"Datos de la tabla 'uber_trips' eliminados correctamente.\")\n",
    "else:\n",
    "    print(\"Borrado deshabilitado. No se realizaron cambios.\")\n",
    "\n",
    "# =========================\n",
    "# Crear Keyspace (si no existe)\n",
    "# =========================\n",
    "session.execute(f\"\"\"\n",
    "CREATE KEYSPACE IF NOT EXISTS {KEYSPACE}\n",
    "WITH replication = {{\n",
    "    'class': 'SimpleStrategy',\n",
    "    'replication_factor': {REPLICATION_FACTOR}\n",
    "}};\n",
    "\"\"\")\n",
    "\n",
    "session.set_keyspace(KEYSPACE)\n",
    "\n",
    "# =========================\n",
    "# Crear tabla (si no existe)\n",
    "# =========================\n",
    "session.execute(f\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS {TABLE} (\n",
    "    index_trip int PRIMARY KEY,\n",
    "    hvfhs_license_num text,\n",
    "    dispatching_base_num text,\n",
    "    originating_base_num text,\n",
    "    request_datetime timestamp,\n",
    "    on_scene_datetime timestamp,\n",
    "    pickup_datetime timestamp,\n",
    "    dropoff_datetime timestamp,\n",
    "    pulocationid int,\n",
    "    dolocationid int,\n",
    "    trip_miles float,\n",
    "    trip_time int,\n",
    "    base_passenger_fare float,\n",
    "    tolls float,\n",
    "    bcf float,\n",
    "    sales_tax float,\n",
    "    congestion_surcharge float,\n",
    "    airport_fee float,\n",
    "    tips float,\n",
    "    driver_pay float,\n",
    "    shared_request_flag text,\n",
    "    shared_match_flag text,\n",
    "    access_a_ride_flag text,\n",
    "    wav_request_flag text,\n",
    "    wav_match_flag text,\n",
    "    hour float,\n",
    "    year float,\n",
    "    month float,\n",
    "    day float,\n",
    "    on_time_pickup int\n",
    ");\n",
    "\"\"\")\n",
    "\n",
    "print(\"Keyspace y tabla verificados.\")\n",
    "\n",
    "# =========================\n",
    "# Verificar si la tabla ya tiene datos\n",
    "# =========================\n",
    "row_check = session.execute(\n",
    "    f\"SELECT index_trip FROM {TABLE} LIMIT 1\"\n",
    ").one()\n",
    "if row_check:\n",
    "    print(\"La tabla ya contiene datos. No se realiza inserción.\")\n",
    "else:\n",
    "    print(\"Tabla vacía. Iniciando carga de datos desde Spark...\")\n",
    "\n",
    "    (\n",
    "        raw_trips\n",
    "        .repartition(max(1, raw_trips.count() // BATCH_SIZE))\n",
    "        .write\n",
    "        .format(\"org.apache.spark.sql.cassandra\")\n",
    "        .mode(\"append\")\n",
    "        .options(table=TABLE, keyspace=KEYSPACE)\n",
    "        .save()\n",
    "    )\n",
    "\n",
    "    print(\"Datos cargados correctamente en Cassandra.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad064363-a973-43d4-8c94-07c0e5c0d8c2",
   "metadata": {},
   "source": [
    "#### 3.4 Consulta de la tabla en Cassandra\n",
    "\n",
    "En esta etapa se valida la conexión con **Apache Cassandra** mediante la lectura de los datos almacenados desde Apache Spark. Se realiza la consulta de la tabla `uber_trips`, perteneciente al keyspace `trips`, y los resultados se cargan en un DataFrame denominado `df_raw`. Finalmente, se visualizan las primeras diez filas con el objetivo de verificar la correcta persistencia y recuperación de la información.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4042a371-1eda-4032-aa88-4a38d1333d04",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.get(\"spark.cassandra.connection.host\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16860a46-7b3e-478c-b25d-1ae42113b2fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "df_raw=spark.read \\\n",
    "    .format(\"org.apache.spark.sql.cassandra\") \\\n",
    "    .options(table=\"uber_trips\", keyspace=\"trips\") \\\n",
    "    .load()\\\n",
    " \n",
    "df_raw.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccf1151f-7dfd-4c55-9d85-e08e12a30441",
   "metadata": {},
   "source": [
    "<b>3.5. Transformación del conjunto de datos</b>\n",
    "<br>\n",
    "Se realiza la lectura del archivo `.csv` con la información de las ubicaciones y se aplican transformaciones de tipo `JOIN` entre tablas. Posteriormente, se ejecutan cálculos entre columnas para determinar la comisión de la plataforma y se generan agregaciones temporales. Finalmente, los datos procesados y agregados son exportados en formato `.csv`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e5189cb-f080-4a50-8e45-84431ac66116",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Lectura del archivo de ubicaciones\n",
    "loc = (\n",
    "    spark.read\n",
    "    .csv(\"taxi_zone_lookup.csv\", header=True, inferSchema=True)\n",
    "    .drop(\"Borough\", \"service_zone\")\n",
    ")\n",
    "\n",
    "loc_pickup = loc.withColumnRenamed(\"Zone\", \"pickup_zone\")\n",
    "loc_dropoff = loc.withColumnRenamed(\"Zone\", \"delivery_zone\")\n",
    "\n",
    "# Limpieza de registros nulos\n",
    "df_clean = df_raw.dropna()\n",
    "\n",
    "# Cálculo de ganancias de la plataforma\n",
    "df_clean = df_clean.withColumn(\n",
    "    \"uber_sales\",\n",
    "    col(\"base_passenger_fare\") + col(\"tips\") - col(\"driver_pay\")\n",
    ")\n",
    "\n",
    "# Join con zona de origen\n",
    "df_clean = (\n",
    "    df_clean\n",
    "    .join(loc_pickup, df_clean.PULocationID == loc_pickup.LocationID, \"left\")\n",
    "    .drop(\"LocationID\")\n",
    ")\n",
    "\n",
    "# Join con zona de destino\n",
    "df_clean = (\n",
    "    df_clean\n",
    "    .join(loc_dropoff, df_clean.DOLocationID == loc_dropoff.LocationID, \"left\")\n",
    "    .drop(\"LocationID\")\n",
    ")\n",
    "\n",
    "# Agregación de ganancias por tiempo\n",
    "df_agg_uber_sales = (\n",
    "    df_clean\n",
    "    .groupBy(\"year\", \"month\", \"day\", \"hour\")\n",
    "    .sum(\"uber_sales\")\n",
    ")\n",
    "\n",
    "# Escritura de resultados\n",
    "df_clean.write.mode(\"append\").option(\"header\", True).csv(\"Proyecto_Final/clean_data.csv\")\n",
    "df_agg_uber_sales.write.mode(\"append\").option(\"header\", True).csv(\"Proyecto_Final/aggregated_data.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e27bc208-e8bc-4578-8f83-b273fd879aaf",
   "metadata": {},
   "source": [
    "<b>3.6 Creación de la tabla en Cassandra de datos procesados</b>\n",
    "<br>\n",
    "Utilizando el mismo keyspace en Cassandra, se crea una nueva tabla denominada `uber_trips_clean`, la cual incluye las columnas generadas tras el proceso de transformación. Posteriormente, la tabla es poblada con el dataframe resultante.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68094485-550f-42f7-8eaf-7e39122e4295",
   "metadata": {},
   "outputs": [],
   "source": [
    "from cassandra.cluster import Cluster\n",
    "from cassandra.policies import RoundRobinPolicy\n",
    "\n",
    "# Configuración Cassandra (capa prod)\n",
    "PROD_CASSANDRA_HOST = \"127.0.0.1\"\n",
    "PROD_CASSANDRA_PORT = 9042\n",
    "PROD_KEYSPACE = \"trips\"\n",
    "\n",
    "# Cluster y sesión para datos procesados (prod)\n",
    "prod_cluster = Cluster(\n",
    "    contact_points=[PROD_CASSANDRA_HOST],\n",
    "    port=PROD_CASSANDRA_PORT,\n",
    "    load_balancing_policy=RoundRobinPolicy()\n",
    ")\n",
    "\n",
    "prod_session = prod_cluster.connect()\n",
    "prod_session.set_keyspace(PROD_KEYSPACE)\n",
    "\n",
    "# Creación de la tabla de datos procesados\n",
    "prod_session.execute(\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS uber_trips_prod (\n",
    "    index_trip int PRIMARY KEY,\n",
    "    hvfhs_license_num text,\n",
    "    dispatching_base_num text,\n",
    "    originating_base_num text,\n",
    "    request_datetime timestamp,\n",
    "    on_scene_datetime timestamp,\n",
    "    pickup_datetime timestamp,\n",
    "    dropoff_datetime timestamp,\n",
    "    PULocationID int,\n",
    "    DOLocationID int,\n",
    "    trip_miles float,\n",
    "    trip_time int,\n",
    "    base_passenger_fare float,\n",
    "    tolls float,\n",
    "    bcf float,\n",
    "    sales_tax float,\n",
    "    congestion_surcharge float,\n",
    "    airport_fee float,\n",
    "    tips float,\n",
    "    driver_pay float,\n",
    "    shared_request_flag text,\n",
    "    shared_match_flag text,\n",
    "    access_a_ride_flag text,\n",
    "    wav_request_flag text,\n",
    "    wav_match_flag text,\n",
    "    hour float,\n",
    "    year float,\n",
    "    month float,\n",
    "    day float,\n",
    "    on_time_pickup int,\n",
    "    uber_sales float,\n",
    "    pickup_zone text,\n",
    "    delivery_zone text\n",
    ");\n",
    "\"\"\")\n",
    "\n",
    "print(\"Tabla 'uber_trips_prod' creada usando la sesión prod_session.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a22e1677-4430-4837-ab71-a93722fdec0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged4.write \\\n",
    "    .format(\"org.apache.spark.sql.cassandra\") \\\n",
    "    .mode(\"append\") \\\n",
    "    .options(table=\"uber_trips_prod\", keyspace=\"trips\") \\\n",
    "    .save()\n",
    "\n",
    "print(\"Datos cargados en Cassandra (tabla uber_trips_prod).\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b3aee35-39bc-466d-bb4e-da5fda9d0676",
   "metadata": {},
   "source": [
    "<b>3.7.a Configuración del productor en Kafka (Tabla cruda)</b>\n",
    "<br>\n",
    "Se establece la conexión con Cassandra para consultar la tabla `uber_trips`, almacenando los registros en una variable. Posteriormente, se importan las librerías necesarias para Kafka y JSON con el fin de serializar los datos y establecer la conexión con el servicio. Finalmente, se empaquetan las columnas requeridas en un diccionario y se envían al tópico `uber_trips` con un intervalo de un segundo entre cada transacción."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bf011b5-24d6-4fad-920c-32c36464208a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from cassandra.cluster import Cluster\n",
    "\n",
    "RAW_CASSANDRA_HOST = \"127.0.0.1\"\n",
    "RAW_KEYSPACE = \"trips\"\n",
    "RAW_TABLE = \"uber_trips\"\n",
    "\n",
    "raw_cluster = Cluster([RAW_CASSANDRA_HOST])\n",
    "raw_session = raw_cluster.connect(RAW_KEYSPACE)\n",
    "\n",
    "raw_rows = raw_session.execute(f\"SELECT * FROM {RAW_TABLE}\")\n",
    "\n",
    "from kafka import KafkaProducer\n",
    "import json\n",
    "from datetime import datetime, timezone\n",
    "import time\n",
    "\n",
    "producer = KafkaProducer(\n",
    "    bootstrap_servers='localhost:9092',\n",
    "    value_serializer=lambda m: json.dumps(m).encode('utf-8')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "479594af-4acd-473c-a821-0661ede2aaff",
   "metadata": {},
   "outputs": [],
   "source": [
    "for row in rows:\n",
    "    row_dict = {\n",
    "        \"index_trip\":row.index_trip,\n",
    "        \"request_datetime\":row.request_datetime.replace(tzinfo=timezone.utc).isoformat(),\n",
    "        \"pickup_datetime\":row.pickup_datetime.replace(tzinfo=timezone.utc).isoformat(),\n",
    "        \"dropoff_datetime\":row.dropoff_datetime.replace(tzinfo=timezone.utc).isoformat(),\n",
    "        \"PULocationID\":row.PULocationID,\n",
    "        \"DOLocationID\":row.DOLocationID,\n",
    "        \"base_passenger_fare\":row.base_passenger_fare,\n",
    "        \"tips\":row.tips,\n",
    "        \"driver_pay\":row.driver_pay,\n",
    "        \"hour\":row.hour,\n",
    "        \"year\":row.year,\n",
    "        \"month\":row.month,\n",
    "        \"day\":row.day,\n",
    "        \"on_time_pickup\":row.on_time_pickup\n",
    "    }\n",
    "    \n",
    "    producer.send('uber_trips',row_dict)\n",
    "    print('enviado:', row_dict)\n",
    "    time.sleep(5)   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4d659e8-9f74-439e-a72e-eafbd3d5ddc1",
   "metadata": {},
   "source": [
    "<b>3.7.b Configuración del productor en Kafka (Tabla procesada)</b>\n",
    "<br>\n",
    "Se establece la conexión con Cassandra para consultar la tabla `uber_trips_prod`, almacenando los registros en una variable. Posteriormente, se importan las librerías necesarias para Kafka y JSON con el fin de serializar los datos y establecer la conexión con el servicio. Finalmente, se empaquetan las columnas requeridas en un diccionario y se envían al tópico `uber_trips_prod` con un intervalo de un segundo entre cada transacción.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caf9357d-7cef-4895-916f-1d076323b5cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from cassandra.cluster import Cluster\n",
    "from kafka import KafkaProducer\n",
    "import json\n",
    "from datetime import timezone\n",
    "import time\n",
    "\n",
    "# Configuración Cassandra (capa PROD)\n",
    "PROD_CASSANDRA_HOST = \"127.0.0.1\"\n",
    "PROD_KEYSPACE = \"trips\"\n",
    "PROD_TABLE = \"uber_trips_prod\"\n",
    "\n",
    "prod_cluster = Cluster([PROD_CASSANDRA_HOST])\n",
    "prod_session = prod_cluster.connect(PROD_KEYSPACE)\n",
    "\n",
    "prod_rows = prod_session.execute(f\"SELECT * FROM {PROD_TABLE}\")\n",
    "\n",
    "# Configuración del productor Kafka\n",
    "producer = KafkaProducer(\n",
    "    bootstrap_servers=\"localhost:9092\",\n",
    "    value_serializer=lambda m: json.dumps(m).encode(\"utf-8\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e51ffbe0-d271-4fde-ad54-16ed98760700",
   "metadata": {},
   "outputs": [],
   "source": [
    "for row in prod_rows:\n",
    "    row_dict = {\n",
    "        \"index_trip\": row.index_trip,\n",
    "        \"request_datetime\": row.request_datetime.replace(tzinfo=timezone.utc).isoformat(),\n",
    "        \"pickup_datetime\": row.pickup_datetime.replace(tzinfo=timezone.utc).isoformat(),\n",
    "        \"dropoff_datetime\": row.dropoff_datetime.replace(tzinfo=timezone.utc).isoformat(),\n",
    "        \"PULocationID\": row.pulocationid,\n",
    "        \"DOLocationID\": row.dolocationid,\n",
    "        \"base_passenger_fare\": row.base_passenger_fare,\n",
    "        \"tips\": row.tips,\n",
    "        \"driver_pay\": row.driver_pay,\n",
    "        \"hour\": row.hour,\n",
    "        \"year\": row.year,\n",
    "        \"month\": row.month,\n",
    "        \"day\": row.day,\n",
    "        \"on_time_pickup\": row.on_time_pickup,\n",
    "        \"uber_sales\": row.uber_sales,\n",
    "        \"pickup_zone\": row.pickup_zone,\n",
    "        \"delivery_zone\": row.delivery_zone\n",
    "    }\n",
    "\n",
    "    producer.send(\"uber_trips_prod\", row_dict)\n",
    "    print(\"enviado:\", row_dict)\n",
    "    time.sleep(5)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
