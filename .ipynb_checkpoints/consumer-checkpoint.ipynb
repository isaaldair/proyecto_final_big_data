{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ffafeb40-1ca4-48f5-8d77-7d5defd9ba81",
   "metadata": {},
   "source": [
    "#### Consumo de datos desde Kafka\n",
    "\n",
    "En esta etapa se realiza el consumo de datos en tiempo real utilizando **Apache Kafka**. Para ello, se importan las librerías necesarias, incluyendo `KafkaConsumer` y PySpark. Posteriormente, se configura el consumidor estableciendo la conexión con el servidor de Kafka y se define el proceso de deserialización de los mensajes, permitiendo transformar los datos recibidos en un formato adecuado para su posterior procesamiento y análisis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96633409-d717-4b9b-9321-3cc652eba105",
   "metadata": {},
   "source": [
    "#### Almacenamiento y procesamiento de datos\n",
    "\n",
    "En esta etapa se realiza el consumo de datos provenientes de **Apache Kafka** mediante el uso de `KafkaConsumer`. A partir de los mensajes recibidos, se construye un DataFrame que permite almacenar los datos y aplicar las transformaciones requeridas para el análisis.\n",
    "\n",
    "Los datos provenientes de Kafka se leen dentro de un bucle de consumo continuo y se almacenan temporalmente en una estructura denominada `data_batch`. Con el fin de optimizar el procesamiento, se limita el número de registros por lote, y mediante una lógica condicional se transforma cada lote de datos en un DataFrame de Spark sobre el cual se aplican las siguientes operaciones:\n",
    "\n",
    "- Limpieza de registros con valores nulos.\n",
    "- Unión (*join*) con la tabla de ubicaciones utilizando los identificadores de origen y destino de los viajes.\n",
    "- Cálculo de las ganancias de la plataforma por cada viaje.\n",
    "- Cálculo del acumulado de las ganancias generadas por la plataforma.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5c6449d-6739-4bac-924b-e89b17f5056f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession iniciada\n",
      "Consumidor Kafka suscrito a: ['uber_trips_clean', 'uber_trips_prod']\n",
      "\n",
      "INICIANDO CONSUMO EN TIEMPO REAL\n",
      "\n",
      "Mensaje recibido [uber_trips_clean] Trip ID: 987\n",
      "Mensaje recibido [uber_trips_clean] Trip ID: 878\n",
      "Mensaje recibido [uber_trips_prod] Trip ID: 987\n",
      "Mensaje recibido [uber_trips_prod] Trip ID: 878\n",
      "Mensaje recibido [uber_trips_clean] Trip ID: 110\n",
      "Mensaje recibido [uber_trips_prod] Trip ID: 110\n",
      "Mensaje recibido [uber_trips_clean] Trip ID: 91\n",
      "Mensaje recibido [uber_trips_prod] Trip ID: 91\n",
      "Mensaje recibido [uber_trips_clean] Trip ID: 128\n",
      "Mensaje recibido [uber_trips_prod] Trip ID: 128\n",
      "\n",
      " Batch completo (10 registros) 00:57:03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV agregado escrito correctamente\n",
      "Ruta: /home/admin/Proyectos/ProyectoFInal/output_realtime_analysis/aggregated_data\n",
      "\n",
      "Top Zonas por Ventas (Batch)\n",
      "+------------------------+-----------------+----+\n",
      "|pickup_zone             |total_sales_zone |hour|\n",
      "+------------------------+-----------------+----+\n",
      "|Financial District North|7.65000057220459 |NULL|\n",
      "|Financial District North|7.65000057220459 |NULL|\n",
      "|Kips Bay                |5.970000267028809|NULL|\n",
      "+------------------------+-----------------+----+\n",
      "\n",
      "--------------------------------------------------\n",
      "Mensaje recibido [uber_trips_clean] Trip ID: 363\n",
      "Mensaje recibido [uber_trips_prod] Trip ID: 363\n",
      "Mensaje recibido [uber_trips_prod] Trip ID: 251\n",
      "Mensaje recibido [uber_trips_clean] Trip ID: 251\n",
      "Mensaje recibido [uber_trips_prod] Trip ID: 744\n",
      "Mensaje recibido [uber_trips_clean] Trip ID: 744\n",
      "Mensaje recibido [uber_trips_prod] Trip ID: 778\n",
      "Mensaje recibido [uber_trips_clean] Trip ID: 778\n",
      "Mensaje recibido [uber_trips_prod] Trip ID: 819\n",
      "Mensaje recibido [uber_trips_clean] Trip ID: 819\n",
      "\n",
      " Batch completo (10 registros) 00:57:28\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV agregado escrito correctamente\n",
      "Ruta: /home/admin/Proyectos/ProyectoFInal/output_realtime_analysis/aggregated_data\n",
      "\n",
      "Top Zonas por Ventas (Batch)\n",
      "+-------------------------+------------------+----+\n",
      "|pickup_zone              |total_sales_zone  |hour|\n",
      "+-------------------------+------------------+----+\n",
      "|Crown Heights North      |1.4200000762939453|NULL|\n",
      "|Crown Heights North      |1.4200000762939453|NULL|\n",
      "|Williamsburg (North Side)|1.3400006294250488|NULL|\n",
      "+-------------------------+------------------+----+\n",
      "\n",
      "--------------------------------------------------\n",
      "Mensaje recibido [uber_trips_prod] Trip ID: 310\n",
      "Mensaje recibido [uber_trips_clean] Trip ID: 310\n"
     ]
    }
   ],
   "source": [
    "from kafka import KafkaConsumer\n",
    "import json\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import (\n",
    "    StructType, StructField,\n",
    "    IntegerType, StringType, FloatType\n",
    ")\n",
    "from pyspark.sql.functions import col, sum, first\n",
    "import time\n",
    "from datetime import datetime\n",
    "import os\n",
    "\n",
    "# =====================================================\n",
    "# 1. CONFIGURACIÓN GENERAL\n",
    "# =====================================================\n",
    "\n",
    "KAFKA_BROKER = '100.68.89.127:9092'\n",
    "TOPICS = ['uber_trips_clean', 'uber_trips_prod']\n",
    "CONSUMER_GROUP = 'uber_realtime_analysis'\n",
    "SIZE_BATCH = 10\n",
    "\n",
    "REALTIME_OUTPUT_DIR = \"output_realtime_analysis\"\n",
    "CLEAN_DIR = f\"{REALTIME_OUTPUT_DIR}/clean_data\"\n",
    "AGG_DIR = f\"{REALTIME_OUTPUT_DIR}/aggregated_data\"\n",
    "\n",
    "os.makedirs(CLEAN_DIR, exist_ok=True)\n",
    "os.makedirs(AGG_DIR, exist_ok=True)\n",
    "\n",
    "# =====================================================\n",
    "# 2. ESQUEMA DE DATOS\n",
    "# =====================================================\n",
    "\n",
    "ENRICHED_SCHEMA = StructType([\n",
    "    StructField(\"index_trip\", IntegerType(), True),\n",
    "    StructField(\"request_datetime\", StringType(), True),\n",
    "    StructField(\"pickup_datetime\", StringType(), True),\n",
    "    StructField(\"dropoff_datetime\", StringType(), True),\n",
    "    StructField(\"pulocationid\", IntegerType(), True),\n",
    "    StructField(\"dolocationid\", IntegerType(), True),\n",
    "    StructField(\"base_passenger_fare\", FloatType(), True),\n",
    "    StructField(\"tips\", FloatType(), True),\n",
    "    StructField(\"driver_pay\", FloatType(), True),\n",
    "    StructField(\"hour\", FloatType(), True),\n",
    "    StructField(\"year\", FloatType(), True),\n",
    "    StructField(\"month\", FloatType(), True),\n",
    "    StructField(\"day\", FloatType(), True),\n",
    "    StructField(\"on_time_pickup\", IntegerType(), True),\n",
    "\n",
    "    # Enriquecidas\n",
    "    StructField(\"uber_sales\", FloatType(), True),\n",
    "    StructField(\"pickup_zone\", StringType(), True),\n",
    "    StructField(\"delivery_zone\", StringType(), True),\n",
    "    StructField(\"source\", StringType(), True),\n",
    "    StructField(\"sent_at\", StringType(), True),\n",
    "])\n",
    "\n",
    "# =====================================================\n",
    "# 3. INICIALIZACIÓN SPARK Y KAFKA\n",
    "# =====================================================\n",
    "\n",
    "spark = (\n",
    "    SparkSession.builder\n",
    "    .appName(\"KafkaTripsRealtimeProcessor\")\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "print(\"SparkSession iniciada\")\n",
    "\n",
    "consumer = KafkaConsumer(\n",
    "    *TOPICS,\n",
    "    bootstrap_servers=KAFKA_BROKER,\n",
    "    auto_offset_reset='earliest',\n",
    "    group_id=CONSUMER_GROUP,\n",
    "    value_deserializer=lambda m: json.loads(m.decode('utf-8')),\n",
    "    consumer_timeout_ms=10000\n",
    ")\n",
    "\n",
    "print(f\"Consumidor Kafka suscrito a: {TOPICS}\")\n",
    "print(\"\\nINICIANDO CONSUMO EN TIEMPO REAL\\n\")\n",
    "\n",
    "# =====================================================\n",
    "# 4. BUCLE PRINCIPAL\n",
    "# =====================================================\n",
    "\n",
    "data_batch = []\n",
    "\n",
    "try:\n",
    "    for message in consumer:\n",
    "\n",
    "        if message.value is None or not isinstance(message.value, dict):\n",
    "            continue\n",
    "\n",
    "        data_batch.append(message.value)\n",
    "\n",
    "        print(\n",
    "            f\"Mensaje recibido [{message.topic}] \"\n",
    "            f\"Trip ID: {message.value.get('index_trip', 'N/A')}\"\n",
    "        )\n",
    "\n",
    "        # -------------------------------------------------\n",
    "        # PROCESAR BATCH\n",
    "        # -------------------------------------------------\n",
    "        if len(data_batch) >= SIZE_BATCH:\n",
    "\n",
    "            print(\n",
    "                f\"\\n Batch completo \"\n",
    "                f\"({len(data_batch)} registros) \"\n",
    "                f\"{datetime.now().strftime('%H:%M:%S')}\"\n",
    "            )\n",
    "\n",
    "            df = spark.createDataFrame(data_batch, ENRICHED_SCHEMA)\n",
    "\n",
    "            # ------------------------------\n",
    "            # AGREGACIÓN\n",
    "            # ------------------------------\n",
    "            df_agg_zone = (\n",
    "                df.groupBy(\"pickup_zone\", \"source\")\n",
    "                .agg(\n",
    "                    sum(\"uber_sales\").alias(\"total_sales_zone\"),\n",
    "                    first(\"year\").alias(\"year\"),\n",
    "                    first(\"hour\").alias(\"hour\")\n",
    "                )\n",
    "                .filter(col(\"total_sales_zone\") > 0)\n",
    "                .orderBy(col(\"total_sales_zone\").desc())\n",
    "            )\n",
    "\n",
    "            # ------------------------------\n",
    "            # ESCRITURA CLEAN DATA\n",
    "            # ------------------------------\n",
    "            df.coalesce(1).write.mode(\"append\").option(\n",
    "                \"header\", True\n",
    "            ).csv(CLEAN_DIR)\n",
    "\n",
    "            # ------------------------------\n",
    "            # ESCRITURA AGREGADOS (SOLO SI HAY DATOS)\n",
    "            # ------------------------------\n",
    "            if df_agg_zone.count() > 0:\n",
    "                df_agg_zone.coalesce(1).write.mode(\"append\").option(\n",
    "                    \"header\", True\n",
    "                ).csv(AGG_DIR)\n",
    "\n",
    "                print(\"CSV agregado escrito correctamente\")\n",
    "                print(f\"Ruta: {os.path.abspath(AGG_DIR)}\")\n",
    "            else:\n",
    "                print(\"Batch sin datos agregables — no se escribe CSV\")\n",
    "\n",
    "            # ------------------------------\n",
    "            # DEBUG VISUAL\n",
    "            # ------------------------------\n",
    "            print(\"\\nTop Zonas por Ventas (Batch)\")\n",
    "            df_agg_zone.select(\n",
    "                \"pickup_zone\", \"total_sales_zone\", \"hour\"\n",
    "            ).limit(3).show(truncate=False)\n",
    "\n",
    "            print(\"-\" * 50)\n",
    "\n",
    "            data_batch.clear()\n",
    "            time.sleep(1)\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"ERROR CRÍTICO EN CONSUMER: {e}\")\n",
    "\n",
    "finally:\n",
    "    consumer.close()\n",
    "    spark.stop()\n",
    "    print(\"\\nConsumer detenido correctamente\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "132b81d6-3a88-4f5b-8501-f6d5ee309d49",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
